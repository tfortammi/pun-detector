{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Converging meanings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfortammi/pun-detector/blob/master/Converging_meanings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GRUBsfYZJDu2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dappity Dap"
      ]
    },
    {
      "metadata": {
        "id": "jmCZLMFXK__x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Characteristics of Puns\n",
        "* Converging Meanings \n",
        "* Sound \n",
        "* Association\n"
      ]
    },
    {
      "metadata": {
        "id": "lriPsNERLdhM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Target: Converging Meanings\n",
        "\n",
        "We have observed that puns often make use of words that have very similar meanings. For example:\n",
        "\n",
        "'He said I was **average** - but he was just being **mean**.'\n",
        "\n",
        "where 'average' and 'mean' have the same meanings but are expressed differently. \n",
        "\n",
        "___\n",
        "\n",
        "In order to test this, we will do the following:\n",
        "\n",
        "* Step 1: Use Synset to list synonyms of tokens\n",
        "* Step 2: Find common words in Synsets within a sentence\n",
        "* Step 3: Determine correlation between converging meanings & whether a sentence is a pun or not\n",
        "\n",
        "---\n",
        "\n",
        "Import/Download relevant packages:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "62735e92-b2e3-4ae2-b1e3-fc6e02dd1405",
        "id": "a2yw0bHqtbkE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCfLoO_cNw19",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this method, we will use NLTK's WordNet corpus to find the synsets of each token in a sentence.\n",
        "\n",
        "As an example, let's test it out on the word **'plant'** first:"
      ]
    },
    {
      "metadata": {
        "id": "pK56tdcLLVxl",
        "colab_type": "code",
        "outputId": "fa163812-56cd-4a2e-9070-0636212d26e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "word = Word('plant')\n",
        "for i in range(3):\n",
        "    print('Use Case ', i)\n",
        "    print(word.synsets[i])\n",
        "    print(word.definitions[i])\n",
        "    print(word.synsets[i].lemma_names())\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use Case  0\n",
            "Synset('plant.n.01')\n",
            "buildings for carrying on industrial labor\n",
            "['plant', 'works', 'industrial_plant']\n",
            " \n",
            "Use Case  1\n",
            "Synset('plant.n.02')\n",
            "(botany) a living organism lacking the power of locomotion\n",
            "['plant', 'flora', 'plant_life']\n",
            " \n",
            "Use Case  2\n",
            "Synset('plant.n.03')\n",
            "an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
            "['plant']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-hvCZaYRvymW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Through WordNet, the **use cases** (Synsets) of the word \"Plant\" can be found, as well as the **definitions** and **Synonyms** (Lemma Names) as the input.\n",
        "\n",
        "---\n",
        "        \n",
        "           \n",
        "Let's first eyeball how relevant the lemmas of each significant word in a sentence to determining if a sentence is a pun. \n",
        "\n",
        "**The example we will use is: \"The past, the present and the future walked into a bar. It was tense.\"**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aMjxEEmJOyLw",
        "colab_type": "code",
        "outputId": "5983e570-99f4-4ea8-dfda-912dcd714c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# First, importing relevant packages, etc\n",
        "\n",
        "import codecs\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lxir3NmAx4Zf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll need to process the sentence, which includes lemmatizing, filtering out stop words, stripping punctuation and tokenizing the sentence."
      ]
    },
    {
      "metadata": {
        "id": "FTK9plVwOSiN",
        "colab_type": "code",
        "outputId": "8a74d808-8264-4f19-923c-23bfa8800f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1491
        }
      },
      "cell_type": "code",
      "source": [
        "def simpleFilter(sentence):\n",
        "    \n",
        "    '''This function filters out stopwords, lemmatizes, tokenizes, and \n",
        "    strips punctuation from the input sentence and returns the a list of \n",
        "    filtered tokens'''\n",
        "    \n",
        "    filtered_sent = []\n",
        "    \n",
        "    # Strip punctuation\n",
        "    stripped = re.sub(\"[(.)',=?!#@]\", '', sentence)\n",
        "        \n",
        "    # filter out stopwords \n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    # Tokenize\n",
        "    words = word_tokenize(stripped)\n",
        "    \n",
        "    # Lemmatize and Filter out Stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for w in words:\n",
        "        if w not in stop_words:\n",
        "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
        "\n",
        "    return filtered_sent\n",
        "  \n",
        "def printLemmas(word):\n",
        "    \n",
        "    '''This function prints out all synonyms of a given word.'''\n",
        "    \n",
        "    for ss in Word(word).synsets:\n",
        "        print(ss.lemma_names())\n",
        "        \n",
        "\n",
        "# Print \n",
        "\n",
        "s = 'The past, the present and the future walked into a bar. It was tense.'\n",
        "\n",
        "for word in simpleFilter(s):\n",
        "    print(\"Filtered word: '\" + word + \"' and its lemmas:\")\n",
        "    printLemmas(word)\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered word: 'The' and its lemmas:\n",
            "\n",
            "Filtered word: 'past' and its lemmas:\n",
            "['past', 'past_times', 'yesteryear']\n",
            "['past']\n",
            "['past', 'past_tense']\n",
            "['past']\n",
            "['past', 'preceding', 'retiring']\n",
            "['by', 'past']\n",
            "\n",
            "Filtered word: 'present' and its lemmas:\n",
            "['present', 'nowadays']\n",
            "['present']\n",
            "['present', 'present_tense']\n",
            "['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
            "['present', 'represent', 'lay_out']\n",
            "['stage', 'present', 'represent']\n",
            "['present', 'submit']\n",
            "['present', 'pose']\n",
            "['award', 'present']\n",
            "['give', 'gift', 'present']\n",
            "['deliver', 'present']\n",
            "['introduce', 'present', 'acquaint']\n",
            "['portray', 'present']\n",
            "['confront', 'face', 'present']\n",
            "['present']\n",
            "['salute', 'present']\n",
            "['present']\n",
            "['present']\n",
            "\n",
            "Filtered word: 'future' and its lemmas:\n",
            "['future', 'hereafter', 'futurity', 'time_to_come']\n",
            "['future', 'future_tense']\n",
            "['future']\n",
            "['future']\n",
            "['future']\n",
            "['future', 'next', 'succeeding']\n",
            "['future']\n",
            "\n",
            "Filtered word: 'walked' and its lemmas:\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk']\n",
            "['walk', 'take_the_air']\n",
            "\n",
            "Filtered word: 'bar' and its lemmas:\n",
            "['barroom', 'bar', 'saloon', 'ginmill', 'taproom']\n",
            "['bar']\n",
            "['bar']\n",
            "['measure', 'bar']\n",
            "['bar']\n",
            "['prevention', 'bar']\n",
            "['bar']\n",
            "['bar']\n",
            "['legal_profession', 'bar', 'legal_community']\n",
            "['stripe', 'streak', 'bar']\n",
            "['cake', 'bar']\n",
            "['Browning_automatic_rifle', 'BAR']\n",
            "['bar']\n",
            "['bar']\n",
            "['bar']\n",
            "['bar', 'debar', 'exclude']\n",
            "['barricade', 'block', 'blockade', 'stop', 'block_off', 'block_up', 'bar']\n",
            "['banish', 'relegate', 'bar']\n",
            "['bar']\n",
            "\n",
            "Filtered word: 'It' and its lemmas:\n",
            "['information_technology', 'IT']\n",
            "\n",
            "Filtered word: 'tense' and its lemmas:\n",
            "['tense']\n",
            "['strain', 'tense']\n",
            "['tense']\n",
            "['tense', 'tense_up']\n",
            "['tense', 'strain', 'tense_up']\n",
            "['tense']\n",
            "['tense']\n",
            "['tense']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lxl_W9dEvRWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Hypothesis 1: Converging Meaning Pun**\n",
        "\n",
        "We observe that the word 'tense' appears as a synonym of the words 'present', 'past', and 'future'. Since we are exploring puns with converging meanings, **we hypothesise that we are more likely to find words with converging meanings in puns than in non-puns.**\n",
        "\n",
        "---\n",
        "\n",
        "To do this, we first produce a list of unique synonyms of a certain word, excluding the word itself.\n",
        "\n",
        "\n",
        "Let's try this on the word \"plant\"."
      ]
    },
    {
      "metadata": {
        "id": "GZrNVNuoQmdK",
        "colab_type": "code",
        "outputId": "3efd52fa-e560-4e15-a9a3-fdbe57206cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "def create_lemmas(word):\n",
        "    lemmas_list = []\n",
        "    for ss in Word(word).synsets:\n",
        "        lemmas_list.append(ss.lemma_names())\n",
        "    return lemmas_list\n",
        "\n",
        "def process_lemmas(lemmas_list, word):\n",
        "    '''\n",
        "    This function process the lemma list of all the definition of a word\n",
        "    and returns a list of all associated unrepeated words with the word\n",
        "    '''\n",
        "    all_lemmas = []\n",
        "    for each_list in lemmas_list:\n",
        "        for lemma in each_list:\n",
        "            if lemma != word and lemma not in all_lemmas:\n",
        "                all_lemmas.append(lemma)\n",
        "    return all_lemmas\n",
        "\n",
        "\n",
        "print(process_lemmas(create_lemmas('plant'), 'plant'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['works', 'industrial_plant', 'flora', 'plant_life', 'set', 'implant', 'engraft', 'embed', 'imbed', 'establish', 'found', 'constitute', 'institute']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n7PhQGRD4XEU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we have to find out if synonyms of any word in a sentence can be found in the rest of the sentence, and count the number of times this occurs."
      ]
    },
    {
      "metadata": {
        "id": "coVT022UOrwf",
        "colab_type": "code",
        "outputId": "aa80c0be-b964-4b6b-cc51-745cb40e3300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "def common_syn(s):\n",
        "    \n",
        "    '''\n",
        "    This function takes in a sentence, processes and tokenizes it and\n",
        "    prints each significant word and tests if its synonyms can be found\n",
        "    in the rest of the sentence. It prints the pair and returns the\n",
        "    number of pairs found.\n",
        "    '''\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    # Filter the sentence to remove filler words / stopwords\n",
        "    filtered_words = simpleFilter(s)\n",
        "    \n",
        "    for index, word in enumerate(filtered_words):\n",
        "        if word.isalpha():\n",
        "            lemma_list_of_term = process_lemmas(create_lemmas(word),word)\n",
        "\n",
        "            # test if any word in the rest of the sentence appears in the lemma list of current word\n",
        "            for other_word in filtered_words[index+1:]:\n",
        "                if other_word in ' '.join(lemma_list_of_term):\n",
        "                    count += 1\n",
        "                    print(word, other_word)\n",
        "    return count\n",
        "    \n",
        "    \n",
        "s = 'The past, the present and the future walked into a bar. It was tense.'\n",
        "print('The number of synonym pairs in this sentence is',common_syn(s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "past tense\n",
            "present tense\n",
            "future tense\n",
            "The number of synonym pairs in this sentence is 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9rXRmeVvyfbo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to see if this method does work, we will test it out on our list of pre-tagged puns and non-puns where puns are tagged '0' and non-puns are tagged '1'\n",
        "\n",
        "We import the list and apply our function common_syn to it, under the label 'Syn Count'."
      ]
    },
    {
      "metadata": {
        "id": "SDLtb-6fXKWr",
        "colab_type": "code",
        "outputId": "e65b603b-3091-4da6-c8f8-2d508ec9dda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10789
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('puns_final.csv', encoding='latin-1')\n",
        "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
        "df['Syn Count'] = df['Sentence'].apply(common_syn)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tuna fish\n",
            "I le\n",
            "bigger le\n",
            "bed le\n",
            "I le\n",
            "pirate high\n",
            "pirate sea\n",
            "make hit\n",
            "I one\n",
            "Going sound\n",
            "bed sleep\n",
            "After ate\n",
            "said ate\n",
            "turned around\n",
            "broke leg\n",
            "I one\n",
            "got one\n",
            "cannibal eat\n",
            "got make\n",
            "paid make\n",
            "reversing back\n",
            "I one\n",
            "got one\n",
            "got back\n",
            "one I\n",
            "go work\n",
            "Cheap u\n",
            "Thrills u\n",
            "want u\n",
            "post office\n",
            "wear wear\n",
            "look see\n",
            "whistle whistle\n",
            "mad hare\n",
            "Old go\n",
            "die go\n",
            "back second\n",
            "call phone\n",
            "Cell phone\n",
            "mean egg\n",
            "laying egg\n",
            "I number\n",
            "people wash\n",
            "little light\n",
            "seems see\n",
            "door door\n",
            "take make\n",
            "fly fly\n",
            "like like\n",
            "metal met\n",
            "I 5\n",
            "mean end\n",
            "went low\n",
            "wardrobe closet\n",
            "one I\n",
            "punch punch\n",
            "went last\n",
            "Do get\n",
            "know get\n",
            "broth stock\n",
            "cat sick\n",
            "cheese cheese\n",
            "Buffalo Bison\n",
            "Make one\n",
            "call one\n",
            "right -\n",
            "duck put\n",
            "Thieves steal\n",
            "dentist tooth\n",
            "theatrical performance\n",
            "pun play\n",
            "pun word\n",
            "play word\n",
            "average mean\n",
            "In I\n",
            "past tense\n",
            "present tense\n",
            "future tense\n",
            "soda soda\n",
            "running go\n",
            "Better go\n",
            "present tense\n",
            "past tense\n",
            "saw ad\n",
            "happens come\n",
            "Id I\n",
            "Id I\n",
            "know get\n",
            "alarm clock\n",
            "Have eat\n",
            "ever time\n",
            "tried time\n",
            "clock time\n",
            "take make\n",
            "seasoned veteran\n",
            "remember back\n",
            "boomerang back\n",
            "I atom\n",
            "error error\n",
            "error one\n",
            "error one\n",
            "noble man\n",
            "fellow man\n",
            "first first\n",
            "first first\n",
            "first first\n",
            "Two I\n",
            "wood le\n",
            "I one\n",
            "I le\n",
            "took le\n",
            "one le\n",
            "Or go\n",
            "control sure\n",
            "universe universe\n",
            "mind mind\n",
            "got ta\n",
            "like like\n",
            "like like\n",
            "like like\n",
            "like like\n",
            "like like\n",
            "like like\n",
            "know love\n",
            "In I\n",
            "three I\n",
            "word go\n",
            "one I\n",
            "one one\n",
            "man I\n",
            "I one\n",
            "take look\n",
            "walk walk\n",
            "know love\n",
            "Live live\n",
            "I single\n",
            "I single\n",
            "single I\n",
            "Darkness darkness\n",
            "drive drive\n",
            "Hate hate\n",
            "live exist\n",
            "world exist\n",
            "I one\n",
            "believe trust\n",
            "happens go\n",
            "happens go\n",
            "happens fall\n",
            "happens fall\n",
            "go go\n",
            "go one\n",
            "go one\n",
            "right good\n",
            "believe trust\n",
            "fall fall\n",
            "throw away\n",
            "Sail sail\n",
            "trade wind\n",
            "gold old\n",
            "wander old\n",
            "way life\n",
            "fell fall\n",
            "love way\n",
            "Good good\n",
            "dwell live\n",
            "fool fool\n",
            "wise wise\n",
            "wise know\n",
            "wise know\n",
            "gift present\n",
            "gutter u\n",
            "man man\n",
            "failed way\n",
            "love make\n",
            "opposite opposite\n",
            "opposite opposite\n",
            "opposite opposite\n",
            "love life\n",
            "opposite opposite\n",
            "opposite opposite\n",
            "opposite opposite\n",
            "Fairy u\n",
            "Fairy u\n",
            "true u\n",
            "true u\n",
            "tell u\n",
            "tell tell\n",
            "tell u\n",
            "u u\n",
            "dragon dragon\n",
            "exist u\n",
            "tell u\n",
            "dog dog\n",
            "gone go\n",
            "intended think\n",
            "love love\n",
            "love love\n",
            "love way\n",
            "love know\n",
            "love way\n",
            "love intimate\n",
            "love intimate\n",
            "knowing love\n",
            "knowing love\n",
            "knowing way\n",
            "knowing know\n",
            "knowing way\n",
            "knowing intimate\n",
            "knowing intimate\n",
            "love love\n",
            "love way\n",
            "love know\n",
            "love way\n",
            "love intimate\n",
            "love intimate\n",
            "love way\n",
            "love know\n",
            "love way\n",
            "love intimate\n",
            "love intimate\n",
            "way way\n",
            "know way\n",
            "know intimate\n",
            "know intimate\n",
            "loving intimate\n",
            "loving intimate\n",
            "hand hand\n",
            "kill u\n",
            "kill u\n",
            "u u\n",
            "make u\n",
            "Have make\n",
            "Have get\n",
            "love make\n",
            "love get\n",
            "make get\n",
            "open open\n",
            "open heart\n",
            "open heart\n",
            "brain head\n",
            "head steer\n",
            "know know\n",
            "know go\n",
            "know go\n",
            "There place\n",
            "minute second\n",
            "give give\n",
            "love love\n",
            "love make\n",
            "love go\n",
            "love make\n",
            "love go\n",
            "noise make\n",
            "church Christian\n",
            "make make\n",
            "reader read\n",
            "life life\n",
            "life life\n",
            "thousand one\n",
            "life life\n",
            "said read\n",
            "said one\n",
            "take deal\n",
            "great stand\n",
            "great stand\n",
            "stand stand\n",
            "see see\n",
            "eye eye\n",
            "time time\n",
            "truer true\n",
            "truer one\n",
            "true one\n",
            "book book\n",
            "zeal read\n",
            "world human\n",
            "back book\n",
            "living read\n",
            "go away\n",
            "sure enough\n",
            "common common\n",
            "common common\n",
            "common common\n",
            "heart soul\n",
            "Have person\n",
            "fallen turn\n",
            "love one\n",
            "love love\n",
            "person one\n",
            "said one\n",
            "said said\n",
            "one I\n",
            "live live\n",
            "live live\n",
            "hundred hundred\n",
            "hundred one\n",
            "I one\n",
            "live live\n",
            "hundred one\n",
            "one I\n",
            "get get\n",
            "read learn\n",
            "know go\n",
            "live live\n",
            "live well\n",
            "failing fail\n",
            "live well\n",
            "get get\n",
            "soul soul\n",
            "soul body\n",
            "soul body\n",
            "going one\n",
            "hurt suffering\n",
            "got find\n",
            "got one\n",
            "find one\n",
            "reading reading\n",
            "venture guess\n",
            "make make\n",
            "Listen listen\n",
            "child have\n",
            "Listen listen\n",
            "Listen listen\n",
            "Listen listen\n",
            "join fall\n",
            "mutually call\n",
            "television set\n",
            "television go\n",
            "turn go\n",
            "set go\n",
            "set read\n",
            "one one\n",
            "one u\n",
            "door door\n",
            "close u\n",
            "open door\n",
            "open opened\n",
            "open u\n",
            "often u\n",
            "look see\n",
            "look one\n",
            "look u\n",
            "long see\n",
            "long u\n",
            "closed u\n",
            "see u\n",
            "one u\n",
            "opened u\n",
            "love love\n",
            "keep keep\n",
            "Love know\n",
            "dy dy\n",
            "dy dy\n",
            "dy dy\n",
            "dy dy\n",
            "death dy\n",
            "death dy\n",
            "death dy\n",
            "death dy\n",
            "dy dy\n",
            "dy dy\n",
            "dy dy\n",
            "dy dy\n",
            "dy dy\n",
            "betrayal dy\n",
            "betrayal dy\n",
            "dy dy\n",
            "illness dy\n",
            "life live\n",
            "know live\n",
            "Time time\n",
            "wanted sure\n",
            "I sin\n",
            "I sin\n",
            "trouble put\n",
            "coming along\n",
            "go lead\n",
            "love love\n",
            "love life\n",
            "love love\n",
            "love life\n",
            "love love\n",
            "I one\n",
            "one I\n",
            "wish like\n",
            "call phone\n",
            "spirit life\n",
            "Humor humor\n",
            "Humor humor\n",
            "humor humor\n",
            "discovering word\n",
            "ordinary ordinary\n",
            "pas pas\n",
            "pas one\n",
            "passing pas\n",
            "passing one\n",
            "signal speak\n",
            "pas one\n",
            "often much\n",
            "love life\n",
            "brings le\n",
            "tolerance le\n",
            "brings le\n",
            "deeper le\n",
            "Success success\n",
            "seeing eye\n",
            "Life living\n",
            "stop speech\n",
            "speech word\n",
            "Humor u\n",
            "alter u\n",
            "help u\n",
            "Hope Hope\n",
            "Hope Hope\n",
            "change change\n",
            "change change\n",
            "working job\n",
            "Hope Hope\n",
            "change change\n",
            "love honey\n",
            "art art\n",
            "way art\n",
            "way man\n",
            "knowing get\n",
            "going work\n",
            "present give\n",
            "Love together\n",
            "universe universe\n",
            "someone person\n",
            "deserving deserve\n",
            "love love\n",
            "affection affection\n",
            "humorless humor\n",
            "Were live\n",
            "born live\n",
            "Only alone\n",
            "peak summit\n",
            "love way\n",
            "love together\n",
            "look one\n",
            "back one\n",
            "life live\n",
            "course run\n",
            "Science science\n",
            "yet ever\n",
            "yet never\n",
            "change change\n",
            "ever always\n",
            "part art\n",
            "world art\n",
            "cross cross\n",
            "cross get\n",
            "Jesus Christ\n",
            "hope go\n",
            "world humanity\n",
            "cross get\n",
            "Gospel go\n",
            "Gospel Gospel\n",
            "hope go\n",
            "go get\n",
            "moment moment\n",
            "Love make\n",
            "Wit humor\n",
            "world art\n",
            "Success inner\n",
            "Success inner\n",
            "life right\n",
            "life right\n",
            "wait waiting\n",
            "happen come\n",
            "Make make\n",
            "happen come\n",
            "Make make\n",
            "future come\n",
            "Make make\n",
            "Make make\n",
            "love make\n",
            "grace grace\n",
            "come make\n",
            "right right\n",
            "take make\n",
            "make one\n",
            "love make\n",
            "love make\n",
            "art try\n",
            "try say\n",
            "feel feel\n",
            "like like\n",
            "felt felt\n",
            "felt feel\n",
            "felt feel\n",
            "trying say\n",
            "liberate free\n",
            "make make\n",
            "get go\n",
            "morning go\n",
            "want need\n",
            "feel feel\n",
            "may u\n",
            "say join\n",
            "say u\n",
            "dreamer u\n",
            "one I\n",
            "one u\n",
            "one one\n",
            "I u\n",
            "I one\n",
            "hope u\n",
            "join u\n",
            "heart heart\n",
            "business business\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Syn Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You can tune a guitar, but you can't tuna fish...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Two peanuts were walking in a tough neighborho...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If I buy a bigger bed will I have more or less...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The earth's rotation really makes my day.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I told my friend she drew her eyebrows too hig...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Novice pirates make terrible singers because t...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>This is not alcohol, water you thinking?!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Two ropes were walking in a tough neighborhood...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The show was called Spongebob Squarepants but ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I, for one, like Roman numerals.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>My phone has to wear glasses ever since it los...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>The plane flight brought my acrophobia to new ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>I met some aliens from outer space. They were ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>A magic tractor drove down the road and turned...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Going to bed with music on gave him sound sleep.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>How do mountains see? They peak.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Our maintenance guy lost his legs on the job, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>What kind of shoes do ninjas wear? Sneakers.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>After eating the ship, the sea monster said, I...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Simba, you're falling behind. I must ask you t...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>I was addicted to the hokey pokey but I turned...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>If you wear cowboy clothes are you ranch dress...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Why are frogs so happy? They eat whatever bugs...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>The tale of the haunted refrigerator was chill...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>A doctor broke his leg while auditioning for a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>I got a masterÂs degree in being ignored; no ...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>I thought I saw a spider on my laptop, but my ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Why donÂt cannibals eat clowns? Because they ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Why does the man want to buy nine rackets? Cau...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>He couldnÂt work out how to fix the washing m...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>When someone loves you the way they talk about...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>When love and skill work together expect a mas...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>Easter is meant to be a symbol of hope renewal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>To be able to look back upon ones past life wi...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>The course of true love never did run smooth</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Humor is merely tragedy standing on its head w...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Science fiction is any idea that occurs in the...</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>You dont have to be a genius or a visionary or...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>Outside of the cross of Jesus Christ there is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>Any life is made up of a single moment the mom...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>Love is like a beautiful flower which I may no...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>Wit is the lowest form of humor</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>There is nothing in the world of art like the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>Success isnt measured by money or power or soc...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>Infuse your life with action Dont wait for it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>Wrong life cannot be lived rightly</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>The most powerful weapon on earth is the human...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>It is the ability to take a joke not make one ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Every bit of me is devoted to love and art And...</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>There is nothing better than a friend unless i...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>Whats money A man is a success if he gets up i...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>It is difficult to say what is impossible for ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>You have succeeded in life when all you really...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>Love is when he gives you a piece of your soul...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>A humorist is a person who feels bad but who f...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>The most beautiful thing we can experience is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>What a slut time is She screws everybody</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>You may say Im a dreamer but Im not the only o...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>To be successful you have to have your heart i...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>He who has health has hope and he who has hope...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>522 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  P/NP  Syn Count\n",
              "0    You can tune a guitar, but you can't tuna fish...     1          1\n",
              "1    Two peanuts were walking in a tough neighborho...     1          0\n",
              "2    If I buy a bigger bed will I have more or less...     1          4\n",
              "3            The earth's rotation really makes my day.     1          0\n",
              "4    I told my friend she drew her eyebrows too hig...     1          0\n",
              "5    Novice pirates make terrible singers because t...     1          3\n",
              "6            This is not alcohol, water you thinking?!     1          0\n",
              "7    Two ropes were walking in a tough neighborhood...     1          0\n",
              "8    The show was called Spongebob Squarepants but ...     1          0\n",
              "9                     I, for one, like Roman numerals.     1          1\n",
              "10   My phone has to wear glasses ever since it los...     1          0\n",
              "11   The plane flight brought my acrophobia to new ...     1          0\n",
              "12   I met some aliens from outer space. They were ...     1          0\n",
              "13   A magic tractor drove down the road and turned...     1          0\n",
              "14    Going to bed with music on gave him sound sleep.     1          2\n",
              "15                    How do mountains see? They peak.     1          0\n",
              "16   Our maintenance guy lost his legs on the job, ...     1          0\n",
              "17        What kind of shoes do ninjas wear? Sneakers.     1          0\n",
              "18   After eating the ship, the sea monster said, I...     1          2\n",
              "19   Simba, you're falling behind. I must ask you t...     1          0\n",
              "20   I was addicted to the hokey pokey but I turned...     1          1\n",
              "21   If you wear cowboy clothes are you ranch dress...     1          0\n",
              "22   Why are frogs so happy? They eat whatever bugs...     1          0\n",
              "23   The tale of the haunted refrigerator was chill...     1          0\n",
              "24   A doctor broke his leg while auditioning for a...     1          1\n",
              "25   I got a masterÂs degree in being ignored; no ...     1          2\n",
              "26   I thought I saw a spider on my laptop, but my ...     1          0\n",
              "27   Why donÂt cannibals eat clowns? Because they ...     1          1\n",
              "28   Why does the man want to buy nine rackets? Cau...     1          0\n",
              "29   He couldnÂt work out how to fix the washing m...     1          0\n",
              "..                                                 ...   ...        ...\n",
              "492  When someone loves you the way they talk about...     0          1\n",
              "493  When love and skill work together expect a mas...     0          1\n",
              "494  Easter is meant to be a symbol of hope renewal...     0          0\n",
              "495  To be able to look back upon ones past life wi...     0          3\n",
              "496       The course of true love never did run smooth     0          1\n",
              "497  Humor is merely tragedy standing on its head w...     0          0\n",
              "498  Science fiction is any idea that occurs in the...     0          7\n",
              "499  You dont have to be a genius or a visionary or...     0          0\n",
              "500  Outside of the cross of Jesus Christ there is ...     0         10\n",
              "501  Any life is made up of a single moment the mom...     0          1\n",
              "502  Love is like a beautiful flower which I may no...     0          1\n",
              "503                    Wit is the lowest form of humor     0          1\n",
              "504  There is nothing in the world of art like the ...     0          1\n",
              "505  Success isnt measured by money or power or soc...     0          2\n",
              "506  Infuse your life with action Dont wait for it ...     0         14\n",
              "507                 Wrong life cannot be lived rightly     0          0\n",
              "508  The most powerful weapon on earth is the human...     0          0\n",
              "509  It is the ability to take a joke not make one ...     0          2\n",
              "510  Every bit of me is devoted to love and art And...     0         12\n",
              "511  There is nothing better than a friend unless i...     0          0\n",
              "512  Whats money A man is a success if he gets up i...     0          2\n",
              "513  It is difficult to say what is impossible for ...     0          0\n",
              "514  You have succeeded in life when all you really...     0          1\n",
              "515  Love is when he gives you a piece of your soul...     0          0\n",
              "516  A humorist is a person who feels bad but who f...     0          1\n",
              "517  The most beautiful thing we can experience is ...     0          0\n",
              "518           What a slut time is She screws everybody     0          0\n",
              "519  You may say Im a dreamer but Im not the only o...     0         11\n",
              "520  To be successful you have to have your heart i...     0          2\n",
              "521  He who has health has hope and he who has hope...     0          0\n",
              "\n",
              "[522 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "metadata": {
        "id": "UKVUEdxK6b1o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To find out if this method is accurate, we use the correlation between whether the sentence is a pun or not and the Syn Count. "
      ]
    },
    {
      "metadata": {
        "id": "laC4Fb4QYmF2",
        "colab_type": "code",
        "outputId": "427986ae-0468-434a-cf93-f0e698a16aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "corr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Syn Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>P/NP</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.241952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Syn Count</th>\n",
              "      <td>-0.241952</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               P/NP  Syn Count\n",
              "P/NP       1.000000  -0.241952\n",
              "Syn Count -0.241952   1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "metadata": {
        "id": "S-9IxQXp6pIP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this case, it appears the Syn Count is not very highly correlated with whether the sentence is a pun or not...\n",
        "\n",
        "Perhaps we should try a different approach.\n",
        "\n",
        "---\n",
        "\n",
        "Other than the ability to find synonyms, WordNet can also find out a range of other details about a word.  "
      ]
    },
    {
      "metadata": {
        "id": "crXdzXjP-HKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use the words 'happy' and 'cutlery' to see what kind of details WordNet can figure out about a word."
      ]
    },
    {
      "metadata": {
        "id": "XBq094Cc8U0W",
        "colab_type": "code",
        "outputId": "2b70f51f-9a28-43f2-bb15-e9073f2b613f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The following are synonyms of 'happy':\")\n",
        "for x in get_all_synsets('happy'):\n",
        "    print(x)\n",
        "print()\n",
        "print(\"The following are hyponyms (words that are more specific) of 'cutlery':\")\n",
        "for x in get_all_hyponyms('cutlery'):\n",
        "    print(x)\n",
        "print()\n",
        "print(\"The following are similar to 'happy':\")\n",
        "for x in get_all_similar_tos('happy'):\n",
        "    print(x)\n",
        "print()\n",
        "print(\"The following are antonyms (opposite) of 'happy':\")\n",
        "for x in get_all_antonyms('happy'):\n",
        "    print(x)\n",
        "print()\n",
        "print(\"The following are words that should also be seen with 'happy':\")\n",
        "for x in get_all_also_sees('happy'):\n",
        "    print(x)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following are synonyms of 'happy':\n",
            "('happy', 'happy.a.01')\n",
            "('felicitous', 'felicitous.s.02')\n",
            "('happy', 'felicitous.s.02')\n",
            "('glad', 'glad.s.02')\n",
            "('happy', 'glad.s.02')\n",
            "('happy', 'happy.s.04')\n",
            "('well-chosen', 'happy.s.04')\n",
            "\n",
            "The following are hyponyms (words that are more specific) of 'cutlery':\n",
            "('bolt_cutter', 'bolt_cutter.n.01')\n",
            "('cigar_cutter', 'cigar_cutter.n.01')\n",
            "('die', 'die.n.03')\n",
            "('edge_tool', 'edge_tool.n.01')\n",
            "('glass_cutter', 'glass_cutter.n.03')\n",
            "('tile_cutter', 'tile_cutter.n.01')\n",
            "('fork', 'fork.n.01')\n",
            "('spoon', 'spoon.n.01')\n",
            "('Spork', 'spork.n.01')\n",
            "('table_knife', 'table_knife.n.01')\n",
            "\n",
            "The following are similar to 'happy':\n",
            "('blessed', 'blessed.s.06')\n",
            "('blissful', 'blissful.s.01')\n",
            "('bright', 'bright.s.09')\n",
            "('golden', 'golden.s.02')\n",
            "('halcyon', 'golden.s.02')\n",
            "('prosperous', 'golden.s.02')\n",
            "('laughing', 'laughing.s.01')\n",
            "('riant', 'laughing.s.01')\n",
            "('fortunate', 'fortunate.a.01')\n",
            "('willing', 'willing.a.01')\n",
            "('felicitous', 'felicitous.a.01')\n",
            "\n",
            "The following are antonyms (opposite) of 'happy':\n",
            "('unhappy', 'unhappy.a.01')\n",
            "\n",
            "The following are words that should also be seen with 'happy':\n",
            "('cheerful', 'cheerful.a.01')\n",
            "('contented', 'contented.a.01')\n",
            "('content', 'contented.a.01')\n",
            "('elated', 'elated.a.01')\n",
            "('euphoric', 'euphoric.a.01')\n",
            "('felicitous', 'felicitous.a.01')\n",
            "('glad', 'glad.a.01')\n",
            "('joyful', 'joyful.a.01')\n",
            "('joyous', 'joyous.a.01')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxCPM76im7dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The functions below make use of WordNet to yield synonyms, hyponyms, antonyms, words that are similar to as well as words that the WordNet corpus has recorded as \"also sees\"."
      ]
    },
    {
      "metadata": {
        "id": "EnkjHc1Kb6aI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def get_all_synsets(word, pos=None):\n",
        "    for ss in wn.synsets(word):\n",
        "        for lemma in ss.lemma_names():\n",
        "            yield (lemma, ss.name())\n",
        "\n",
        "\n",
        "def get_all_hyponyms(word, pos=None):\n",
        "    for ss in wn.synsets(word, pos=pos):\n",
        "            for hyp in ss.hyponyms():\n",
        "                for lemma in hyp.lemma_names():\n",
        "                    yield (lemma, hyp.name())\n",
        "\n",
        "\n",
        "def get_all_similar_tos(word, pos=None):\n",
        "    for ss in wn.synsets(word):\n",
        "            for sim in ss.similar_tos():\n",
        "                for lemma in sim.lemma_names():\n",
        "                    yield (lemma, sim.name())\n",
        "\n",
        "\n",
        "def get_all_antonyms(word, pos=None):\n",
        "    for ss in wn.synsets(word, pos=None):\n",
        "        for sslema in ss.lemmas():\n",
        "            for antlemma in sslema.antonyms():\n",
        "                    yield (antlemma.name(), antlemma.synset().name())\n",
        "\n",
        "\n",
        "def get_all_also_sees(word, pos=None):\n",
        "        for ss in wn.synsets(word):\n",
        "            for also in ss.also_sees():\n",
        "                for lemma in also.lemma_names():\n",
        "                    yield (lemma, also.name())\n",
        "\n",
        "\n",
        "def get_all_synonyms(word, pos=None):\n",
        "    for x in get_all_synsets(word, pos):\n",
        "        yield (x[0], x[1], 'ss')\n",
        "    for x in get_all_hyponyms(word, pos):\n",
        "        yield (x[0], x[1], 'hyp')\n",
        "    for x in get_all_similar_tos(word, pos):\n",
        "        yield (x[0], x[1], 'sim')\n",
        "    for x in get_all_antonyms(word, pos):\n",
        "        yield (x[0], x[1], 'ant')\n",
        "    for x in get_all_also_sees(word, pos):\n",
        "        yield (x[0], x[1], 'also')\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k2mBXMIL-R8z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's all the categories above words that are **related** to the main word. \n",
        "\n",
        "Now, we want to do the same as we did for the synonym count and define some functions that will find the common related words - not just within the sentence, but also with the related words of the other words in the sentence. "
      ]
    },
    {
      "metadata": {
        "id": "0snqjLX-lKD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example:**\n",
        "\n",
        "'What do you call a belt with a watch on it? A waist of time.'"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L3wnzCQ5gRjO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def related_list(word):\n",
        "    lemma_list = []\n",
        "    for x in get_all_synonyms(word):\n",
        "        lemma_list.append(x)\n",
        "        for y in get_all_synonyms(x[0]):\n",
        "            lemma_list.append(y)\n",
        "    return list(set(lemma_list))\n",
        "\n",
        "def common_related(s):\n",
        "    filtered = simpleFilter(s)\n",
        "    count = 0\n",
        "    for index, word in enumerate(filtered):\n",
        "        related = related_list(word)\n",
        "        for r_set in related:\n",
        "            if r_set[0] in filtered[index+1:]:\n",
        "                count += 1\n",
        "    return count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h17Biokqk0QQ",
        "colab_type": "code",
        "outputId": "6c996f20-fbd4-432b-f715-4d6b484717d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "s = 'What do you call a belt with a watch on it? A waist of time.'\n",
        "\n",
        "filtered = simpleFilter(s)\n",
        "count = 0\n",
        "print('Sentence:',s)\n",
        "print(filtered)\n",
        "print('-----' *10)\n",
        "print()\n",
        "for index, word in enumerate(filtered):\n",
        "    related = related_list(word)\n",
        "    for r_set in related:\n",
        "        if r_set[0] in filtered[index+1:]:\n",
        "            print(\"The word '\" + word + \"' in the sentence is related to '\" + r_set[0] + \"' as\", r_set, \"to mean '\" + wordnet.synset(r_set[1]).definition() +\"'\")\n",
        "            print()\n",
        "            \n",
        "            count += 1\n",
        "print('-----' * 10)\n",
        "print('Number of Related pairs:', count)\n",
        "print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: What do you call a belt with a watch on it? A waist of time.\n",
            "['What', 'call', 'belt', 'watch', 'A', 'waist', 'time']\n",
            "--------------------------------------------------\n",
            "\n",
            "The word 'call' in the sentence is related to 'watch' as ('watch', 'determine.v.08', 'ss') to mean 'find out, learn, or determine with certainty, usually by making an inquiry or other effort'\n",
            "\n",
            "The word 'call' in the sentence is related to 'time' as ('time', 'prison_term.n.01', 'hyp') to mean 'the period of time a prisoner is imprisoned'\n",
            "\n",
            "The word 'call' in the sentence is related to 'watch' as ('watch', 'watch.v.03', 'ss') to mean 'see or watch'\n",
            "\n",
            "The word 'watch' in the sentence is related to 'time' as ('time', 'time.v.03', 'hyp') to mean 'set the speed, duration, or execution of'\n",
            "\n",
            "--------------------------------------------------\n",
            "Number of Related pairs: 4\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3k1fliJg2XnI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we want to apply this to the rest of our data."
      ]
    },
    {
      "metadata": {
        "id": "T9mz_g5plIsT",
        "colab_type": "code",
        "outputId": "79a36c2e-5ee6-4811-d4fb-06571a60d943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "def num_words(s):\n",
        "    sentence = s.split()\n",
        "    num_words = len(sentence)\n",
        "    return num_words\n",
        "\n",
        "df['Related Count'] = df['Sentence'].apply(common_related)\n",
        "df['Length'] = df['Sentence'].apply(num_words)\n",
        "df['Rel Count / Len'] = df['Related Count'] / df['Length']\n",
        "df.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Syn Count</th>\n",
              "      <th>Related Count</th>\n",
              "      <th>Length</th>\n",
              "      <th>Rel Count / Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>You can search throughout the entire universe ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>46</td>\n",
              "      <td>0.695652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>Success comes from knowing that you did your b...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>19</td>\n",
              "      <td>0.842105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>There is nothing I would not do for those who ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>Wit is the lowest form of humor</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>1.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>Friendship  is born at the moment when one man...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>1.409091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  P/NP  Syn Count  \\\n",
              "483  You can search throughout the entire universe ...     0          5   \n",
              "468  Success comes from knowing that you did your b...     0          0   \n",
              "425  There is nothing I would not do for those who ...     0          0   \n",
              "503                    Wit is the lowest form of humor     0          1   \n",
              "302  Friendship  is born at the moment when one man...     0          4   \n",
              "\n",
              "     Related Count  Length  Rel Count / Len  \n",
              "483             32      46         0.695652  \n",
              "468             16      19         0.842105  \n",
              "425              4      28         0.142857  \n",
              "503              8       7         1.142857  \n",
              "302             31      22         1.409091  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "metadata": {
        "id": "uldmmxd4hc_N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a description of the values. "
      ]
    },
    {
      "metadata": {
        "id": "QQS8AyWghNb9",
        "colab_type": "code",
        "outputId": "e62f55c3-8e42-4b8a-b1c0-13f8749c5153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df.describe()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Syn Count</th>\n",
              "      <th>Related Count</th>\n",
              "      <th>Length</th>\n",
              "      <th>Rel Count / Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>522.00000</td>\n",
              "      <td>522.000000</td>\n",
              "      <td>522.000000</td>\n",
              "      <td>522.000000</td>\n",
              "      <td>522.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.50000</td>\n",
              "      <td>0.977011</td>\n",
              "      <td>19.545977</td>\n",
              "      <td>15.040230</td>\n",
              "      <td>0.964886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.50048</td>\n",
              "      <td>2.504401</td>\n",
              "      <td>57.173598</td>\n",
              "      <td>8.784928</td>\n",
              "      <td>1.814452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.50000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.230769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.750000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.093168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>813.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>15.625000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            P/NP   Syn Count  Related Count      Length  Rel Count / Len\n",
              "count  522.00000  522.000000     522.000000  522.000000       522.000000\n",
              "mean     0.50000    0.977011      19.545977   15.040230         0.964886\n",
              "std      0.50048    2.504401      57.173598    8.784928         1.814452\n",
              "min      0.00000    0.000000       0.000000    5.000000         0.000000\n",
              "25%      0.00000    0.000000       0.000000   10.000000         0.000000\n",
              "50%      0.50000    0.000000       3.000000   13.000000         0.230769\n",
              "75%      1.00000    1.000000      16.750000   16.000000         1.093168\n",
              "max      1.00000   32.000000     813.000000   74.000000        15.625000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "metadata": {
        "id": "V9sgFr4Mgi1K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below finds the correlation between the different variables in the data frame. \n",
        "\n",
        "As can be seen, the correlation between whether a sentence is a pun or not and the number of related count pairs is debatable.\n",
        "\n",
        "We also took related count / len of sentence as a longer sentence is more likely to have more related pairs."
      ]
    },
    {
      "metadata": {
        "id": "SDFQNmnfmD1L",
        "colab_type": "code",
        "outputId": "7749a0d1-f6df-4ddb-cda7-c30a140a3ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "corr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Syn Count</th>\n",
              "      <th>Related Count</th>\n",
              "      <th>Length</th>\n",
              "      <th>Rel Count / Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>P/NP</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.241952</td>\n",
              "      <td>-0.200865</td>\n",
              "      <td>-0.381330</td>\n",
              "      <td>-0.154173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Syn Count</th>\n",
              "      <td>-0.241952</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.596042</td>\n",
              "      <td>0.654087</td>\n",
              "      <td>0.406106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Related Count</th>\n",
              "      <td>-0.200865</td>\n",
              "      <td>0.596042</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.598361</td>\n",
              "      <td>0.759567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Length</th>\n",
              "      <td>-0.381330</td>\n",
              "      <td>0.654087</td>\n",
              "      <td>0.598361</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.316411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rel Count / Len</th>\n",
              "      <td>-0.154173</td>\n",
              "      <td>0.406106</td>\n",
              "      <td>0.759567</td>\n",
              "      <td>0.316411</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     P/NP  Syn Count  Related Count    Length  Rel Count / Len\n",
              "P/NP             1.000000  -0.241952      -0.200865 -0.381330        -0.154173\n",
              "Syn Count       -0.241952   1.000000       0.596042  0.654087         0.406106\n",
              "Related Count   -0.200865   0.596042       1.000000  0.598361         0.759567\n",
              "Length          -0.381330   0.654087       0.598361  1.000000         0.316411\n",
              "Rel Count / Len -0.154173   0.406106       0.759567  0.316411         1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "metadata": {
        "id": "BKXuGCUmZuTG",
        "colab_type": "code",
        "outputId": "6ee66620-02b3-4589-b50d-95eb1df29c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "sum_pun = df.loc[df['P/NP'] == 0]['Rel Count / Len'].sum()\n",
        "len_pun = len(df.loc[df['P/NP'] == 0]['Rel Count / Len'])\n",
        "mean_pun = sum_pun / len_pun\n",
        "print('Mean Related count / len for Puns is', mean_pun)\n",
        "\n",
        "sum_non_pun = df.loc[df['P/NP'] == 1]['Rel Count / Len'].sum()\n",
        "len_non_pun = len(df.loc[df['P/NP'] == 1]['Rel Count / Len'])\n",
        "mean_non_pun = sum_non_pun / len_non_pun\n",
        "print('Mean Related count / len for Non-Puns is', mean_non_pun)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Related count / len for Puns is 1.2443563344509985\n",
            "Mean Related count / len for Non-Puns is 0.6854153935930384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fWL8jgk9niVV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll try to turn this correlation into an actionable \"algorithm\" to predict if a sentence is a pun or not. \n",
        "\n",
        "The following is another data set with 60 puns and 100 non-puns."
      ]
    },
    {
      "metadata": {
        "id": "RmFYXOdxLX6g",
        "colab_type": "code",
        "outputId": "d1130e8a-9df6-418c-c13e-d6de85cf38f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('puns_test.csv')\n",
        "test_df['Rel Count'] = test_df['Sentence'].apply(common_related)\n",
        "test_df['Length'] = test_df['Sentence'].apply(num_words)\n",
        "test_df['Rel Count / Len'] = test_df['Rel Count'] / test_df['Length']\n",
        "test_df.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Rel Count</th>\n",
              "      <th>Length</th>\n",
              "      <th>Rel Count / Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What does a clock do when it's hungry? It goes...</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>3.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>There are only three things women need in life...</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A scarecrow says, \"This job isn't for everyone...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>The soldier who survived mustard gas and peppe...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>World War II ended the Great Depression with o...</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>20</td>\n",
              "      <td>1.450000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  P/NP  Rel Count  \\\n",
              "10   What does a clock do when it's hungry? It goes...     0         50   \n",
              "127  There are only three things women need in life...     1         39   \n",
              "4    A scarecrow says, \"This job isn't for everyone...     0          0   \n",
              "34   The soldier who survived mustard gas and peppe...     0          5   \n",
              "145  World War II ended the Great Depression with o...     1         29   \n",
              "\n",
              "     Length  Rel Count / Len  \n",
              "10       13         3.846154  \n",
              "127      13         3.000000  \n",
              "4        14         0.000000  \n",
              "34       13         0.384615  \n",
              "145      20         1.450000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "metadata": {
        "id": "eYPxIGisoGaJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now code the \"algorithm\".\n",
        "\n",
        "To do so, we need to know the threshold that we will use to determine if a sentence is a pun or not, based on the related pair count. \n",
        "\n",
        "Let's try using the mean Rel Count / Len first"
      ]
    },
    {
      "metadata": {
        "id": "o95k48CcujNY",
        "colab_type": "code",
        "outputId": "c3ef3e36-011f-45ed-b727-198bfaca085d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "def rel_count_predict(s, threshold=1.2443563344509985):\n",
        "    rel_count = common_related(s)\n",
        "    rel_count_len = rel_count / num_words(s)\n",
        "    if rel_count_len >= threshold:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "test_df['Predicted by Rel Count / Len'] = test_df['Sentence'].apply(rel_count_predict)\n",
        "test_df.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>P/NP</th>\n",
              "      <th>Rel Count</th>\n",
              "      <th>Length</th>\n",
              "      <th>Rel Count / Len</th>\n",
              "      <th>Predicted by Rel Count / Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>All of my friends are animal people. To me, ca...</td>\n",
              "      <td>1</td>\n",
              "      <td>97</td>\n",
              "      <td>42</td>\n",
              "      <td>2.309524</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>What should you do if you’re cold? Stand in th...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>You can never stop and as older people, we hav...</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>35</td>\n",
              "      <td>2.285714</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>One goes through school, college, medical scho...</td>\n",
              "      <td>1</td>\n",
              "      <td>74</td>\n",
              "      <td>22</td>\n",
              "      <td>3.363636</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>By recollecting the pleasures I have had forme...</td>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>33</td>\n",
              "      <td>1.454545</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  P/NP  Rel Count  \\\n",
              "133  All of my friends are animal people. To me, ca...     1         97   \n",
              "29   What should you do if you’re cold? Stand in th...     0          0   \n",
              "74   You can never stop and as older people, we hav...     1         80   \n",
              "90   One goes through school, college, medical scho...     1         74   \n",
              "95   By recollecting the pleasures I have had forme...     1         48   \n",
              "\n",
              "     Length  Rel Count / Len  Predicted by Rel Count / Len  \n",
              "133      42         2.309524                             0  \n",
              "29       14         0.000000                             1  \n",
              "74       35         2.285714                             0  \n",
              "90       22         3.363636                             0  \n",
              "95       33         1.454545                             0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "metadata": {
        "id": "4be0J-u0q6sL",
        "colab_type": "code",
        "outputId": "3df86d89-cb93-4f66-d89c-b16d26e8b3fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "y_true = test_df['P/NP']\n",
        "y_pred = test_df['Predicted by Rel Count / Len']\n",
        "\n",
        "\n",
        "sns.set()\n",
        "sns.heatmap(cm_df, annot=True)\n",
        "\n",
        "print('The accuracy rate is', accuracy_score(y_true, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy rate is 0.36075949367088606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAFKCAYAAABme+rbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTvg/raloEnCmLo0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f6crK1q6AEdk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the code above, it appears that we're getting a 36% accuracy rate."
      ]
    }
  ]
}