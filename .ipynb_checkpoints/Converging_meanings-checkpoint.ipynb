{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tfortammi/pun-detector/blob/master/Converging_meanings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRUBsfYZJDu2"
   },
   "source": [
    "# Dappity Dap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmCZLMFXK__x"
   },
   "source": [
    "### Characteristics of Puns\n",
    "* Converging Meanings \n",
    "* Sound \n",
    "* Association\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lriPsNERLdhM"
   },
   "source": [
    "### Target: Converging Meanings\n",
    "\n",
    "We have observed that puns often make use of words that have very similar meanings. For example:\n",
    "\n",
    "'He said I was **average** - but he was just being **mean**.'\n",
    "\n",
    "where 'average' and 'mean' have the same meanings but are expressed differently. \n",
    "\n",
    "___\n",
    "\n",
    "In order to test this, we will do the following:\n",
    "\n",
    "* Step 1: Use Synset to list synonyms of tokens\n",
    "* Step 2: Find common words in Synsets within a sentence\n",
    "* Step 3: Determine correlation between converging meanings & whether a sentence is a pun or not\n",
    "\n",
    "---\n",
    "\n",
    "Import/Download relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "a2yw0bHqtbkE",
    "outputId": "62735e92-b2e3-4ae2-b1e3-fc6e02dd1405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tammi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCfLoO_cNw19"
   },
   "source": [
    "For this method, we will use NLTK's WordNet corpus to find the synsets of each token in a sentence.\n",
    "\n",
    "As an example, let's test it out on the word **'plant'** first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "pK56tdcLLVxl",
    "outputId": "fa163812-56cd-4a2e-9070-0636212d26e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Case  0\n",
      "Synset('plant.n.01')\n",
      "buildings for carrying on industrial labor\n",
      "['plant', 'works', 'industrial_plant']\n",
      " \n",
      "Use Case  1\n",
      "Synset('plant.n.02')\n",
      "(botany) a living organism lacking the power of locomotion\n",
      "['plant', 'flora', 'plant_life']\n",
      " \n",
      "Use Case  2\n",
      "Synset('plant.n.03')\n",
      "an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
      "['plant']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "word = Word('plant')\n",
    "for i in range(3):\n",
    "    print('Use Case ', i)\n",
    "    print(word.synsets[i])\n",
    "    print(word.definitions[i])\n",
    "    print(word.synsets[i].lemma_names())\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hvCZaYRvymW"
   },
   "source": [
    "Through WordNet, the **use cases** (Synsets) of the word \"Plant\" can be found, as well as the **definitions** and **Synonyms** (Lemma Names) as the input.\n",
    "\n",
    "---\n",
    "        \n",
    "           \n",
    "Let's first eyeball how relevant the lemmas of each significant word in a sentence to determining if a sentence is a pun. \n",
    "\n",
    "**The example we will use is: \"The past, the present and the future walked into a bar. It was tense.\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "aMjxEEmJOyLw",
    "outputId": "5983e570-99f4-4ea8-dfda-912dcd714c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tammi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tammi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# First, importing relevant packages, etc\n",
    "\n",
    "import codecs\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxir3NmAx4Zf"
   },
   "source": [
    "We'll need to process the sentence, which includes lemmatizing, filtering out stop words, stripping punctuation and tokenizing the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1491
    },
    "colab_type": "code",
    "id": "FTK9plVwOSiN",
    "outputId": "8a74d808-8264-4f19-923c-23bfa8800f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered word: 'The' and its lemmas:\n",
      "\n",
      "Filtered word: 'past' and its lemmas:\n",
      "['past', 'past_times', 'yesteryear']\n",
      "['past']\n",
      "['past', 'past_tense']\n",
      "['past']\n",
      "['past', 'preceding', 'retiring']\n",
      "['by', 'past']\n",
      "\n",
      "Filtered word: 'present' and its lemmas:\n",
      "['present', 'nowadays']\n",
      "['present']\n",
      "['present', 'present_tense']\n",
      "['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "['present', 'represent', 'lay_out']\n",
      "['stage', 'present', 'represent']\n",
      "['present', 'submit']\n",
      "['present', 'pose']\n",
      "['award', 'present']\n",
      "['give', 'gift', 'present']\n",
      "['deliver', 'present']\n",
      "['introduce', 'present', 'acquaint']\n",
      "['portray', 'present']\n",
      "['confront', 'face', 'present']\n",
      "['present']\n",
      "['salute', 'present']\n",
      "['present']\n",
      "['present']\n",
      "\n",
      "Filtered word: 'future' and its lemmas:\n",
      "['future', 'hereafter', 'futurity', 'time_to_come']\n",
      "['future', 'future_tense']\n",
      "['future']\n",
      "['future']\n",
      "['future']\n",
      "['future', 'next', 'succeeding']\n",
      "['future']\n",
      "\n",
      "Filtered word: 'walked' and its lemmas:\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk', 'take_the_air']\n",
      "\n",
      "Filtered word: 'bar' and its lemmas:\n",
      "['barroom', 'bar', 'saloon', 'ginmill', 'taproom']\n",
      "['bar']\n",
      "['bar']\n",
      "['measure', 'bar']\n",
      "['bar']\n",
      "['prevention', 'bar']\n",
      "['bar']\n",
      "['bar']\n",
      "['legal_profession', 'bar', 'legal_community']\n",
      "['stripe', 'streak', 'bar']\n",
      "['cake', 'bar']\n",
      "['Browning_automatic_rifle', 'BAR']\n",
      "['bar']\n",
      "['bar']\n",
      "['bar']\n",
      "['bar', 'debar', 'exclude']\n",
      "['barricade', 'block', 'blockade', 'stop', 'block_off', 'block_up', 'bar']\n",
      "['banish', 'relegate', 'bar']\n",
      "['bar']\n",
      "\n",
      "Filtered word: 'It' and its lemmas:\n",
      "['information_technology', 'IT']\n",
      "\n",
      "Filtered word: 'tense' and its lemmas:\n",
      "['tense']\n",
      "['strain', 'tense']\n",
      "['tense']\n",
      "['tense', 'tense_up']\n",
      "['tense', 'strain', 'tense_up']\n",
      "['tense']\n",
      "['tense']\n",
      "['tense']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simpleFilter(sentence):\n",
    "    \n",
    "    '''This function filters out stopwords, lemmatizes, tokenizes, and \n",
    "    strips punctuation from the input sentence and returns the a list of \n",
    "    filtered tokens'''\n",
    "    \n",
    "    filtered_sent = []\n",
    "    \n",
    "    # Strip punctuation\n",
    "    stripped = re.sub(\"[(.)',=?!#@]\", '', sentence)\n",
    "        \n",
    "    # filter out stopwords \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(stripped)\n",
    "    \n",
    "    # Lemmatize and Filter out Stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    return filtered_sent\n",
    "  \n",
    "def printLemmas(word):\n",
    "    \n",
    "    '''This function prints out all synonyms of a given word.'''\n",
    "    \n",
    "    for ss in Word(word).synsets:\n",
    "        print(ss.lemma_names())\n",
    "        \n",
    "\n",
    "# Print \n",
    "\n",
    "s = 'The past, the present and the future walked into a bar. It was tense.'\n",
    "\n",
    "for word in simpleFilter(s):\n",
    "    print(\"Filtered word: '\" + word + \"' and its lemmas:\")\n",
    "    printLemmas(word)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxl_W9dEvRWZ"
   },
   "source": [
    "---\n",
    "## **Hypothesis 1: Converging Meaning Pun**\n",
    "\n",
    "We observe that the word 'tense' appears as a synonym of the words 'present', 'past', and 'future'. Since we are exploring puns with converging meanings, **we hypothesise that we are more likely to find words with converging meanings in puns than in non-puns.**\n",
    "\n",
    "---\n",
    "\n",
    "To do this, we first produce a list of unique synonyms of a certain word, excluding the word itself.\n",
    "\n",
    "\n",
    "Let's try this on the word \"plant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "GZrNVNuoQmdK",
    "outputId": "3efd52fa-e560-4e15-a9a3-fdbe57206cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'industrial_plant', 'flora', 'plant_life', 'set', 'implant', 'engraft', 'embed', 'imbed', 'establish', 'found', 'constitute', 'institute']\n"
     ]
    }
   ],
   "source": [
    "def create_lemmas(word):\n",
    "    lemmas_list = []\n",
    "    for ss in Word(word).synsets:\n",
    "        lemmas_list.append(ss.lemma_names())\n",
    "    return lemmas_list\n",
    "\n",
    "def process_lemmas(lemmas_list, word):\n",
    "    '''\n",
    "    This function process the lemma list of all the definition of a word\n",
    "    and returns a list of all associated unrepeated words with the word\n",
    "    '''\n",
    "    all_lemmas = []\n",
    "    for each_list in lemmas_list:\n",
    "        for lemma in each_list:\n",
    "            if lemma != word and lemma not in all_lemmas:\n",
    "                all_lemmas.append(lemma)\n",
    "    return all_lemmas\n",
    "\n",
    "\n",
    "print(process_lemmas(create_lemmas('plant'), 'plant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7PhQGRD4XEU"
   },
   "source": [
    "Next, we have to find out if synonyms of any word in a sentence can be found in the rest of the sentence, and count the number of times this occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "coVT022UOrwf",
    "outputId": "aa80c0be-b964-4b6b-cc51-745cb40e3300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past tense\n",
      "present tense\n",
      "future tense\n",
      "The number of synonym pairs in this sentence is 3\n"
     ]
    }
   ],
   "source": [
    "def print_common_syn(s):\n",
    "    \n",
    "    '''\n",
    "    This function takes in a sentence, processes and tokenizes it and\n",
    "    prints each significant word and tests if its synonyms can be found\n",
    "    in the rest of the sentence. It prints the pair and returns the\n",
    "    number of pairs found.\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # Filter the sentence to remove filler words / stopwords\n",
    "    filtered_words = simpleFilter(s)\n",
    "    \n",
    "    for index, word in enumerate(filtered_words):\n",
    "        if word.isalpha():\n",
    "            lemma_list_of_term = process_lemmas(create_lemmas(word),word)\n",
    "\n",
    "            # test if any word in the rest of the sentence appears in the lemma list of current word\n",
    "            for other_word in filtered_words[index+1:]:\n",
    "                if other_word in ' '.join(lemma_list_of_term):\n",
    "                    count += 1\n",
    "                    print(word, other_word)\n",
    "    return count\n",
    "    \n",
    "    \n",
    "s = 'The past, the present and the future walked into a bar. It was tense.'\n",
    "print('The number of synonym pairs in this sentence is',print_common_syn(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_syn(s):\n",
    "    \n",
    "    '''\n",
    "    This function takes in a sentence, processes and tokenizes it and\n",
    "    prints each significant word and tests if its synonyms can be found\n",
    "    in the rest of the sentence. It prints the pair and returns the\n",
    "    number of pairs found.\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # Filter the sentence to remove filler words / stopwords\n",
    "    filtered_words = simpleFilter(s)\n",
    "    \n",
    "    for index, word in enumerate(filtered_words):\n",
    "        if word.isalpha():\n",
    "            lemma_list_of_term = process_lemmas(create_lemmas(word),word)\n",
    "\n",
    "            # test if any word in the rest of the sentence appears in the lemma list of current word\n",
    "            for other_word in filtered_words[index+1:]:\n",
    "                if other_word in ' '.join(lemma_list_of_term):\n",
    "                    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rXRmeVvyfbo"
   },
   "source": [
    "In order to see if this method does work, we will test it out on our list of pre-tagged puns and non-puns where puns are tagged '0' and non-puns are tagged '1'\n",
    "\n",
    "We import the list and apply our function common_syn to it, under the label 'Syn Count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10789
    },
    "colab_type": "code",
    "id": "SDLtb-6fXKWr",
    "outputId": "e65b603b-3091-4da6-c8f8-2d508ec9dda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuna fish\n",
      "I le\n",
      "bigger le\n",
      "bed le\n",
      "I le\n",
      "pirate high\n",
      "pirate sea\n",
      "make hit\n",
      "I one\n",
      "Going sound\n",
      "bed sleep\n",
      "After ate\n",
      "said ate\n",
      "turned around\n",
      "broke leg\n",
      "I one\n",
      "got one\n",
      "cannibal eat\n",
      "got make\n",
      "paid make\n",
      "reversing back\n",
      "I one\n",
      "got one\n",
      "got back\n",
      "one I\n",
      "go work\n",
      "Cheap u\n",
      "Thrills u\n",
      "want u\n",
      "post office\n",
      "wear wear\n",
      "look see\n",
      "whistle whistle\n",
      "mad hare\n",
      "Old go\n",
      "die go\n",
      "back second\n",
      "call phone\n",
      "Cell phone\n",
      "mean egg\n",
      "laying egg\n",
      "I number\n",
      "people wash\n",
      "little light\n",
      "seems see\n",
      "door door\n",
      "take make\n",
      "fly fly\n",
      "like like\n",
      "metal met\n",
      "I 5\n",
      "mean end\n",
      "went low\n",
      "wardrobe closet\n",
      "one I\n",
      "punch punch\n",
      "went last\n",
      "Do get\n",
      "know get\n",
      "broth stock\n",
      "cat sick\n",
      "cheese cheese\n",
      "Buffalo Bison\n",
      "Make one\n",
      "call one\n",
      "right -\n",
      "duck put\n",
      "Thieves steal\n",
      "dentist tooth\n",
      "theatrical performance\n",
      "pun play\n",
      "pun word\n",
      "play word\n",
      "average mean\n",
      "In I\n",
      "past tense\n",
      "present tense\n",
      "future tense\n",
      "soda soda\n",
      "running go\n",
      "Better go\n",
      "present tense\n",
      "past tense\n",
      "saw ad\n",
      "happens come\n",
      "Id I\n",
      "Id I\n",
      "know get\n",
      "alarm clock\n",
      "Have eat\n",
      "ever time\n",
      "tried time\n",
      "clock time\n",
      "take make\n",
      "seasoned veteran\n",
      "remember back\n",
      "boomerang back\n",
      "I atom\n",
      "error error\n",
      "error one\n",
      "error one\n",
      "noble man\n",
      "fellow man\n",
      "first first\n",
      "first first\n",
      "first first\n",
      "Two I\n",
      "wood le\n",
      "I one\n",
      "I le\n",
      "took le\n",
      "one le\n",
      "Or go\n",
      "control sure\n",
      "universe universe\n",
      "mind mind\n",
      "got ta\n",
      "like like\n",
      "like like\n",
      "like like\n",
      "like like\n",
      "like like\n",
      "like like\n",
      "know love\n",
      "In I\n",
      "three I\n",
      "word go\n",
      "one I\n",
      "one one\n",
      "man I\n",
      "I one\n",
      "take look\n",
      "walk walk\n",
      "know love\n",
      "Live live\n",
      "I single\n",
      "I single\n",
      "single I\n",
      "Darkness darkness\n",
      "drive drive\n",
      "Hate hate\n",
      "live exist\n",
      "world exist\n",
      "I one\n",
      "believe trust\n",
      "happens go\n",
      "happens go\n",
      "happens fall\n",
      "happens fall\n",
      "go go\n",
      "go one\n",
      "go one\n",
      "right good\n",
      "believe trust\n",
      "fall fall\n",
      "throw away\n",
      "Sail sail\n",
      "trade wind\n",
      "gold old\n",
      "wander old\n",
      "way life\n",
      "fell fall\n",
      "love way\n",
      "Good good\n",
      "dwell live\n",
      "fool fool\n",
      "wise wise\n",
      "wise know\n",
      "wise know\n",
      "gift present\n",
      "gutter u\n",
      "man man\n",
      "failed way\n",
      "love make\n",
      "opposite opposite\n",
      "opposite opposite\n",
      "opposite opposite\n",
      "love life\n",
      "opposite opposite\n",
      "opposite opposite\n",
      "opposite opposite\n",
      "Fairy u\n",
      "Fairy u\n",
      "true u\n",
      "true u\n",
      "tell u\n",
      "tell tell\n",
      "tell u\n",
      "u u\n",
      "dragon dragon\n",
      "exist u\n",
      "tell u\n",
      "dog dog\n",
      "gone go\n",
      "intended think\n",
      "love love\n",
      "love love\n",
      "love way\n",
      "love know\n",
      "love way\n",
      "love intimate\n",
      "love intimate\n",
      "knowing love\n",
      "knowing love\n",
      "knowing way\n",
      "knowing know\n",
      "knowing way\n",
      "knowing intimate\n",
      "knowing intimate\n",
      "love love\n",
      "love way\n",
      "love know\n",
      "love way\n",
      "love intimate\n",
      "love intimate\n",
      "love way\n",
      "love know\n",
      "love way\n",
      "love intimate\n",
      "love intimate\n",
      "way way\n",
      "know way\n",
      "know intimate\n",
      "know intimate\n",
      "loving intimate\n",
      "loving intimate\n",
      "hand hand\n",
      "kill u\n",
      "kill u\n",
      "u u\n",
      "make u\n",
      "Have make\n",
      "Have get\n",
      "love make\n",
      "love get\n",
      "make get\n",
      "open open\n",
      "open heart\n",
      "open heart\n",
      "brain head\n",
      "head steer\n",
      "know know\n",
      "know go\n",
      "know go\n",
      "There place\n",
      "minute second\n",
      "give give\n",
      "love love\n",
      "love make\n",
      "love go\n",
      "love make\n",
      "love go\n",
      "noise make\n",
      "church Christian\n",
      "make make\n",
      "reader read\n",
      "life life\n",
      "life life\n",
      "thousand one\n",
      "life life\n",
      "said read\n",
      "said one\n",
      "take deal\n",
      "great stand\n",
      "great stand\n",
      "stand stand\n",
      "see see\n",
      "eye eye\n",
      "time time\n",
      "truer true\n",
      "truer one\n",
      "true one\n",
      "book book\n",
      "zeal read\n",
      "world human\n",
      "back book\n",
      "living read\n",
      "go away\n",
      "sure enough\n",
      "common common\n",
      "common common\n",
      "common common\n",
      "heart soul\n",
      "Have person\n",
      "fallen turn\n",
      "love one\n",
      "love love\n",
      "person one\n",
      "said one\n",
      "said said\n",
      "one I\n",
      "live live\n",
      "live live\n",
      "hundred hundred\n",
      "hundred one\n",
      "I one\n",
      "live live\n",
      "hundred one\n",
      "one I\n",
      "get get\n",
      "read learn\n",
      "know go\n",
      "live live\n",
      "live well\n",
      "failing fail\n",
      "live well\n",
      "get get\n",
      "soul soul\n",
      "soul body\n",
      "soul body\n",
      "going one\n",
      "hurt suffering\n",
      "got find\n",
      "got one\n",
      "find one\n",
      "reading reading\n",
      "venture guess\n",
      "make make\n",
      "Listen listen\n",
      "child have\n",
      "Listen listen\n",
      "Listen listen\n",
      "Listen listen\n",
      "join fall\n",
      "mutually call\n",
      "television set\n",
      "television go\n",
      "turn go\n",
      "set go\n",
      "set read\n",
      "one one\n",
      "one u\n",
      "door door\n",
      "close u\n",
      "open door\n",
      "open opened\n",
      "open u\n",
      "often u\n",
      "look see\n",
      "look one\n",
      "look u\n",
      "long see\n",
      "long u\n",
      "closed u\n",
      "see u\n",
      "one u\n",
      "opened u\n",
      "love love\n",
      "keep keep\n",
      "Love know\n",
      "dy dy\n",
      "dy dy\n",
      "dy dy\n",
      "dy dy\n",
      "death dy\n",
      "death dy\n",
      "death dy\n",
      "death dy\n",
      "dy dy\n",
      "dy dy\n",
      "dy dy\n",
      "dy dy\n",
      "dy dy\n",
      "betrayal dy\n",
      "betrayal dy\n",
      "dy dy\n",
      "illness dy\n",
      "life live\n",
      "know live\n",
      "Time time\n",
      "wanted sure\n",
      "I sin\n",
      "I sin\n",
      "trouble put\n",
      "coming along\n",
      "go lead\n",
      "love love\n",
      "love life\n",
      "love love\n",
      "love life\n",
      "love love\n",
      "I one\n",
      "one I\n",
      "wish like\n",
      "call phone\n",
      "spirit life\n",
      "Humor humor\n",
      "Humor humor\n",
      "humor humor\n",
      "discovering word\n",
      "ordinary ordinary\n",
      "pas pas\n",
      "pas one\n",
      "passing pas\n",
      "passing one\n",
      "signal speak\n",
      "pas one\n",
      "often much\n",
      "love life\n",
      "brings le\n",
      "tolerance le\n",
      "brings le\n",
      "deeper le\n",
      "Success success\n",
      "seeing eye\n",
      "Life living\n",
      "stop speech\n",
      "speech word\n",
      "Humor u\n",
      "alter u\n",
      "help u\n",
      "Hope Hope\n",
      "Hope Hope\n",
      "change change\n",
      "change change\n",
      "working job\n",
      "Hope Hope\n",
      "change change\n",
      "love honey\n",
      "art art\n",
      "way art\n",
      "way man\n",
      "knowing get\n",
      "going work\n",
      "present give\n",
      "Love together\n",
      "universe universe\n",
      "someone person\n",
      "deserving deserve\n",
      "love love\n",
      "affection affection\n",
      "humorless humor\n",
      "Were live\n",
      "born live\n",
      "Only alone\n",
      "peak summit\n",
      "love way\n",
      "love together\n",
      "look one\n",
      "back one\n",
      "life live\n",
      "course run\n",
      "Science science\n",
      "yet ever\n",
      "yet never\n",
      "change change\n",
      "ever always\n",
      "part art\n",
      "world art\n",
      "cross cross\n",
      "cross get\n",
      "Jesus Christ\n",
      "hope go\n",
      "world humanity\n",
      "cross get\n",
      "Gospel go\n",
      "Gospel Gospel\n",
      "hope go\n",
      "go get\n",
      "moment moment\n",
      "Love make\n",
      "Wit humor\n",
      "world art\n",
      "Success inner\n",
      "Success inner\n",
      "life right\n",
      "life right\n",
      "wait waiting\n",
      "happen come\n",
      "Make make\n",
      "happen come\n",
      "Make make\n",
      "future come\n",
      "Make make\n",
      "Make make\n",
      "love make\n",
      "grace grace\n",
      "come make\n",
      "right right\n",
      "take make\n",
      "make one\n",
      "love make\n",
      "love make\n",
      "art try\n",
      "try say\n",
      "feel feel\n",
      "like like\n",
      "felt felt\n",
      "felt feel\n",
      "felt feel\n",
      "trying say\n",
      "liberate free\n",
      "make make\n",
      "get go\n",
      "morning go\n",
      "want need\n",
      "feel feel\n",
      "may u\n",
      "say join\n",
      "say u\n",
      "dreamer u\n",
      "one I\n",
      "one u\n",
      "one one\n",
      "I u\n",
      "I one\n",
      "hope u\n",
      "join u\n",
      "heart heart\n",
      "business business\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You can tune a guitar, but you can't tuna fish...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two peanuts were walking in a tough neighborho...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If I buy a bigger bed will I have more or less...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The earth's rotation really makes my day.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I told my friend she drew her eyebrows too hig...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Novice pirates make terrible singers because t...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This is not alcohol, water you thinking?!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Two ropes were walking in a tough neighborhood...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The show was called Spongebob Squarepants but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I, for one, like Roman numerals.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>My phone has to wear glasses ever since it los...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The plane flight brought my acrophobia to new ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I met some aliens from outer space. They were ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A magic tractor drove down the road and turned...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Going to bed with music on gave him sound sleep.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How do mountains see? They peak.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Our maintenance guy lost his legs on the job, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What kind of shoes do ninjas wear? Sneakers.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>After eating the ship, the sea monster said, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Simba, you're falling behind. I must ask you t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I was addicted to the hokey pokey but I turned...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>If you wear cowboy clothes are you ranch dress...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Why are frogs so happy? They eat whatever bugs...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The tale of the haunted refrigerator was chill...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A doctor broke his leg while auditioning for a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I got a masterÂs degree in being ignored; no ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I thought I saw a spider on my laptop, but my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Why donÂt cannibals eat clowns? Because they ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Why does the man want to buy nine rackets? Cau...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>He couldnÂt work out how to fix the washing m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>When someone loves you the way they talk about...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>When love and skill work together expect a mas...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Easter is meant to be a symbol of hope renewal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>To be able to look back upon ones past life wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>The course of true love never did run smooth</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Humor is merely tragedy standing on its head w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Science fiction is any idea that occurs in the...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>You dont have to be a genius or a visionary or...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Outside of the cross of Jesus Christ there is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Any life is made up of a single moment the mom...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Love is like a beautiful flower which I may no...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Wit is the lowest form of humor</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>There is nothing in the world of art like the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Success isnt measured by money or power or soc...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Infuse your life with action Dont wait for it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Wrong life cannot be lived rightly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>The most powerful weapon on earth is the human...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>It is the ability to take a joke not make one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Every bit of me is devoted to love and art And...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>There is nothing better than a friend unless i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Whats money A man is a success if he gets up i...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>It is difficult to say what is impossible for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>You have succeeded in life when all you really...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Love is when he gives you a piece of your soul...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>A humorist is a person who feels bad but who f...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>The most beautiful thing we can experience is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>What a slut time is She screws everybody</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>You may say Im a dreamer but Im not the only o...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>To be successful you have to have your heart i...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>He who has health has hope and he who has hope...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>521 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  Syn Count\n",
       "0    You can tune a guitar, but you can't tuna fish...     1          1\n",
       "1    Two peanuts were walking in a tough neighborho...     1          0\n",
       "2    If I buy a bigger bed will I have more or less...     1          4\n",
       "3            The earth's rotation really makes my day.     1          0\n",
       "4    I told my friend she drew her eyebrows too hig...     1          0\n",
       "5    Novice pirates make terrible singers because t...     1          3\n",
       "6            This is not alcohol, water you thinking?!     1          0\n",
       "7    Two ropes were walking in a tough neighborhood...     1          0\n",
       "8    The show was called Spongebob Squarepants but ...     1          0\n",
       "9                     I, for one, like Roman numerals.     1          1\n",
       "10   My phone has to wear glasses ever since it los...     1          0\n",
       "11   The plane flight brought my acrophobia to new ...     1          0\n",
       "12   I met some aliens from outer space. They were ...     1          0\n",
       "13   A magic tractor drove down the road and turned...     1          0\n",
       "14    Going to bed with music on gave him sound sleep.     1          2\n",
       "15                    How do mountains see? They peak.     1          0\n",
       "16   Our maintenance guy lost his legs on the job, ...     1          0\n",
       "17        What kind of shoes do ninjas wear? Sneakers.     1          0\n",
       "18   After eating the ship, the sea monster said, I...     1          2\n",
       "19   Simba, you're falling behind. I must ask you t...     1          0\n",
       "20   I was addicted to the hokey pokey but I turned...     1          1\n",
       "21   If you wear cowboy clothes are you ranch dress...     1          0\n",
       "22   Why are frogs so happy? They eat whatever bugs...     1          0\n",
       "23   The tale of the haunted refrigerator was chill...     1          0\n",
       "24   A doctor broke his leg while auditioning for a...     1          1\n",
       "25   I got a masterÂs degree in being ignored; no ...     1          2\n",
       "26   I thought I saw a spider on my laptop, but my ...     1          0\n",
       "27   Why donÂt cannibals eat clowns? Because they ...     1          1\n",
       "28   Why does the man want to buy nine rackets? Cau...     1          0\n",
       "29   He couldnÂt work out how to fix the washing m...     1          0\n",
       "..                                                 ...   ...        ...\n",
       "491  When someone loves you the way they talk about...     0          1\n",
       "492  When love and skill work together expect a mas...     0          1\n",
       "493  Easter is meant to be a symbol of hope renewal...     0          0\n",
       "494  To be able to look back upon ones past life wi...     0          3\n",
       "495       The course of true love never did run smooth     0          1\n",
       "496  Humor is merely tragedy standing on its head w...     0          0\n",
       "497  Science fiction is any idea that occurs in the...     0          7\n",
       "498  You dont have to be a genius or a visionary or...     0          0\n",
       "499  Outside of the cross of Jesus Christ there is ...     0         10\n",
       "500  Any life is made up of a single moment the mom...     0          1\n",
       "501  Love is like a beautiful flower which I may no...     0          1\n",
       "502                    Wit is the lowest form of humor     0          1\n",
       "503  There is nothing in the world of art like the ...     0          1\n",
       "504  Success isnt measured by money or power or soc...     0          2\n",
       "505  Infuse your life with action Dont wait for it ...     0         14\n",
       "506                 Wrong life cannot be lived rightly     0          0\n",
       "507  The most powerful weapon on earth is the human...     0          0\n",
       "508  It is the ability to take a joke not make one ...     0          2\n",
       "509  Every bit of me is devoted to love and art And...     0         12\n",
       "510  There is nothing better than a friend unless i...     0          0\n",
       "511  Whats money A man is a success if he gets up i...     0          2\n",
       "512  It is difficult to say what is impossible for ...     0          0\n",
       "513  You have succeeded in life when all you really...     0          1\n",
       "514  Love is when he gives you a piece of your soul...     0          0\n",
       "515  A humorist is a person who feels bad but who f...     0          1\n",
       "516  The most beautiful thing we can experience is ...     0          0\n",
       "517           What a slut time is She screws everybody     0          0\n",
       "518  You may say Im a dreamer but Im not the only o...     0         11\n",
       "519  To be successful you have to have your heart i...     0          2\n",
       "520  He who has health has hope and he who has hope...     0          0\n",
       "\n",
       "[521 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('puns_final.csv', encoding='latin-1')\n",
    "df['Syn Count'] = df['Sentence'].apply(common_syn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKVUEdxK6b1o"
   },
   "source": [
    "To find out if this method is accurate, we use the correlation between whether the sentence is a pun or not and the Syn Count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "laC4Fb4QYmF2",
    "outputId": "427986ae-0468-434a-cf93-f0e698a16aea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P/NP</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.24147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syn Count</th>\n",
       "      <td>-0.24147</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P/NP  Syn Count\n",
       "P/NP       1.00000   -0.24147\n",
       "Syn Count -0.24147    1.00000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-9IxQXp6pIP"
   },
   "source": [
    "In this case, it appears the Syn Count is not very highly correlated with whether the sentence is a pun or not...\n",
    "\n",
    "Perhaps we should try a different approach.\n",
    "\n",
    "---\n",
    "\n",
    "Other than the ability to find synonyms, WordNet can also find out a range of other details about a word.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crXdzXjP-HKv"
   },
   "source": [
    "Let's use the words 'happy' and 'cutlery' to see what kind of details WordNet can figure out about a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "colab_type": "code",
    "id": "XBq094Cc8U0W",
    "outputId": "2b70f51f-9a28-43f2-bb15-e9073f2b613f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are synonyms of 'happy':\n",
      "('happy', 'happy.a.01')\n",
      "('felicitous', 'felicitous.s.02')\n",
      "('happy', 'felicitous.s.02')\n",
      "('glad', 'glad.s.02')\n",
      "('happy', 'glad.s.02')\n",
      "('happy', 'happy.s.04')\n",
      "('well-chosen', 'happy.s.04')\n",
      "\n",
      "The following are hyponyms (words that are more specific) of 'cutlery':\n",
      "('bolt_cutter', 'bolt_cutter.n.01')\n",
      "('cigar_cutter', 'cigar_cutter.n.01')\n",
      "('die', 'die.n.03')\n",
      "('edge_tool', 'edge_tool.n.01')\n",
      "('glass_cutter', 'glass_cutter.n.03')\n",
      "('tile_cutter', 'tile_cutter.n.01')\n",
      "('fork', 'fork.n.01')\n",
      "('spoon', 'spoon.n.01')\n",
      "('Spork', 'spork.n.01')\n",
      "('table_knife', 'table_knife.n.01')\n",
      "\n",
      "The following are similar to 'happy':\n",
      "('blessed', 'blessed.s.06')\n",
      "('blissful', 'blissful.s.01')\n",
      "('bright', 'bright.s.09')\n",
      "('golden', 'golden.s.02')\n",
      "('halcyon', 'golden.s.02')\n",
      "('prosperous', 'golden.s.02')\n",
      "('laughing', 'laughing.s.01')\n",
      "('riant', 'laughing.s.01')\n",
      "('fortunate', 'fortunate.a.01')\n",
      "('willing', 'willing.a.01')\n",
      "('felicitous', 'felicitous.a.01')\n",
      "\n",
      "The following are antonyms (opposite) of 'happy':\n",
      "('unhappy', 'unhappy.a.01')\n",
      "\n",
      "The following are words that should also be seen with 'happy':\n",
      "('cheerful', 'cheerful.a.01')\n",
      "('contented', 'contented.a.01')\n",
      "('content', 'contented.a.01')\n",
      "('elated', 'elated.a.01')\n",
      "('euphoric', 'euphoric.a.01')\n",
      "('felicitous', 'felicitous.a.01')\n",
      "('glad', 'glad.a.01')\n",
      "('joyful', 'joyful.a.01')\n",
      "('joyous', 'joyous.a.01')\n"
     ]
    }
   ],
   "source": [
    "print(\"The following are synonyms of 'happy':\")\n",
    "for x in get_all_synsets('happy'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are hyponyms (words that are more specific) of 'cutlery':\")\n",
    "for x in get_all_hyponyms('cutlery'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are similar to 'happy':\")\n",
    "for x in get_all_similar_tos('happy'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are antonyms (opposite) of 'happy':\")\n",
    "for x in get_all_antonyms('happy'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are words that should also be seen with 'happy':\")\n",
    "for x in get_all_also_sees('happy'):\n",
    "    print(x)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxCPM76im7dv"
   },
   "source": [
    "The functions below make use of WordNet to yield synonyms, hyponyms, antonyms, words that are similar to as well as words that the WordNet corpus has recorded as \"also sees\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnkjHc1Kb6aI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_all_synsets(word, pos=None):\n",
    "    for ss in wn.synsets(word):\n",
    "        for lemma in ss.lemma_names():\n",
    "            yield (lemma, ss.name())\n",
    "\n",
    "\n",
    "def get_all_hyponyms(word, pos=None):\n",
    "    for ss in wn.synsets(word, pos=pos):\n",
    "            for hyp in ss.hyponyms():\n",
    "                for lemma in hyp.lemma_names():\n",
    "                    yield (lemma, hyp.name())\n",
    "\n",
    "\n",
    "def get_all_similar_tos(word, pos=None):\n",
    "    for ss in wn.synsets(word):\n",
    "            for sim in ss.similar_tos():\n",
    "                for lemma in sim.lemma_names():\n",
    "                    yield (lemma, sim.name())\n",
    "\n",
    "\n",
    "def get_all_antonyms(word, pos=None):\n",
    "    for ss in wn.synsets(word, pos=None):\n",
    "        for sslema in ss.lemmas():\n",
    "            for antlemma in sslema.antonyms():\n",
    "                    yield (antlemma.name(), antlemma.synset().name())\n",
    "\n",
    "\n",
    "def get_all_also_sees(word, pos=None):\n",
    "        for ss in wn.synsets(word):\n",
    "            for also in ss.also_sees():\n",
    "                for lemma in also.lemma_names():\n",
    "                    yield (lemma, also.name())\n",
    "\n",
    "\n",
    "def get_all_synonyms(word, pos=None):\n",
    "    for x in get_all_synsets(word, pos):\n",
    "        yield (x[0], x[1], 'ss')\n",
    "    for x in get_all_hyponyms(word, pos):\n",
    "        yield (x[0], x[1], 'hyp')\n",
    "    for x in get_all_similar_tos(word, pos):\n",
    "        yield (x[0], x[1], 'sim')\n",
    "    for x in get_all_antonyms(word, pos):\n",
    "        yield (x[0], x[1], 'ant')\n",
    "    for x in get_all_also_sees(word, pos):\n",
    "        yield (x[0], x[1], 'also')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2mBXMIL-R8z"
   },
   "source": [
    "Let's all the categories above words that are **related** to the main word. \n",
    "\n",
    "Now, we want to do the same as we did for the synonym count and define some functions that will find the common related words - not just within the sentence, but also with the related words of the other words in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0snqjLX-lKD-"
   },
   "source": [
    "**Example:**\n",
    "\n",
    "'What do you call a belt with a watch on it? A waist of time.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3wnzCQ5gRjO"
   },
   "outputs": [],
   "source": [
    "def related_list(word):\n",
    "    lemma_list = []\n",
    "    for x in get_all_synonyms(word):\n",
    "        lemma_list.append(x)\n",
    "        for y in get_all_synonyms(x[0]):\n",
    "            lemma_list.append(y)\n",
    "    return list(set(lemma_list))\n",
    "\n",
    "def common_related(s):\n",
    "    filtered = simpleFilter(s)\n",
    "    count = 0\n",
    "    for index, word in enumerate(filtered):\n",
    "        related = related_list(word)\n",
    "        for r_set in related:\n",
    "            if r_set[0] in filtered[index+1:]:\n",
    "                count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "h17Biokqk0QQ",
    "outputId": "6c996f20-fbd4-432b-f715-4d6b484717d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: What do you call a belt with a watch on it? A waist of time.\n",
      "['What', 'call', 'belt', 'watch', 'A', 'waist', 'time']\n",
      "--------------------------------------------------\n",
      "\n",
      "The word 'call' in the sentence is related to 'time' as ('time', 'prison_term.n.01', 'hyp') to mean 'the period of time a prisoner is imprisoned'\n",
      "\n",
      "The word 'call' in the sentence is related to 'watch' as ('watch', 'watch.v.03', 'ss') to mean 'see or watch'\n",
      "\n",
      "The word 'call' in the sentence is related to 'watch' as ('watch', 'determine.v.08', 'ss') to mean 'find out, learn, or determine with certainty, usually by making an inquiry or other effort'\n",
      "\n",
      "The word 'watch' in the sentence is related to 'time' as ('time', 'time.v.03', 'hyp') to mean 'set the speed, duration, or execution of'\n",
      "\n",
      "--------------------------------------------------\n",
      "Number of Related pairs: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = 'What do you call a belt with a watch on it? A waist of time.'\n",
    "\n",
    "filtered = simpleFilter(s)\n",
    "count = 0\n",
    "print('Sentence:',s)\n",
    "print(filtered)\n",
    "print('-----' *10)\n",
    "print()\n",
    "for index, word in enumerate(filtered):\n",
    "    related = related_list(word)\n",
    "    for r_set in related:\n",
    "        if r_set[0] in filtered[index+1:]:\n",
    "            print(\"The word '\" + word + \"' in the sentence is related to '\" + r_set[0] + \"' as\", r_set, \"to mean '\" + wordnet.synset(r_set[1]).definition() +\"'\")\n",
    "            print()\n",
    "            \n",
    "            count += 1\n",
    "print('-----' * 10)\n",
    "print('Number of Related pairs:', count)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3k1fliJg2XnI"
   },
   "source": [
    "Now we want to apply this to the rest of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "T9mz_g5plIsT",
    "outputId": "79a36c2e-5ee6-4811-d4fb-06571a60d943"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "      <th>Related Count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <th>Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The sore mummy needed a Cairo-practor</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>I had a real problem making a hard-boiled egg ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>What do you call a girl with one leg that's sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>It is our choices Harry that show what we trul...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Space is the breath of art</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  Syn Count  \\\n",
       "72               The sore mummy needed a Cairo-practor     1          0   \n",
       "106  I had a real problem making a hard-boiled egg ...     1          0   \n",
       "172  What do you call a girl with one leg that's sh...     1          1   \n",
       "326  It is our choices Harry that show what we trul...     0          0   \n",
       "490                         Space is the breath of art     0          0   \n",
       "\n",
       "     Related Count  Length  Rel Count / Len  Syn Count / Len  \n",
       "72               0       6         0.000000         0.000000  \n",
       "106              4      15         0.266667         0.000000  \n",
       "172              0      15         0.000000         0.066667  \n",
       "326              1      16         0.062500         0.000000  \n",
       "490              0       6         0.000000         0.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_words(s):\n",
    "    sentence = s.split()\n",
    "    num_words = len(sentence)\n",
    "    return num_words\n",
    "\n",
    "df['Related Count'] = df['Sentence'].apply(common_related)\n",
    "df['Length'] = df['Sentence'].apply(num_words)\n",
    "df['Rel Count / Len'] = df['Related Count'] / df['Length']\n",
    "df['Syn Count / Len'] = df['Syn Count'] / df['Length']\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uldmmxd4hc_N"
   },
   "source": [
    "Here is a description of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "QQS8AyWghNb9",
    "outputId": "e62f55c3-8e42-4b8a-b1c0-13f8749c5153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "      <th>Related Count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <th>Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>521.00000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.49904</td>\n",
       "      <td>0.978887</td>\n",
       "      <td>19.583493</td>\n",
       "      <td>15.053743</td>\n",
       "      <td>0.966738</td>\n",
       "      <td>0.047487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50048</td>\n",
       "      <td>2.506441</td>\n",
       "      <td>57.222115</td>\n",
       "      <td>8.787939</td>\n",
       "      <td>1.815702</td>\n",
       "      <td>0.087244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.095238</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>813.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>0.548387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            P/NP   Syn Count  Related Count      Length  Rel Count / Len  \\\n",
       "count  521.00000  521.000000     521.000000  521.000000       521.000000   \n",
       "mean     0.49904    0.978887      19.583493   15.053743         0.966738   \n",
       "std      0.50048    2.506441      57.222115    8.787939         1.815702   \n",
       "min      0.00000    0.000000       0.000000    5.000000         0.000000   \n",
       "25%      0.00000    0.000000       0.000000   10.000000         0.000000   \n",
       "50%      0.00000    0.000000       3.000000   13.000000         0.230769   \n",
       "75%      1.00000    1.000000      17.000000   16.000000         1.095238   \n",
       "max      1.00000   32.000000     813.000000   74.000000        15.625000   \n",
       "\n",
       "       Syn Count / Len  \n",
       "count       521.000000  \n",
       "mean          0.047487  \n",
       "std           0.087244  \n",
       "min           0.000000  \n",
       "25%           0.000000  \n",
       "50%           0.000000  \n",
       "75%           0.071429  \n",
       "max           0.548387  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9sgFr4Mgi1K"
   },
   "source": [
    "The code below finds the correlation between the different variables in the data frame. \n",
    "\n",
    "As can be seen, the correlation between whether a sentence is a pun or not and the number of related count pairs is debatable.\n",
    "\n",
    "We also took related count / len of sentence as a longer sentence is more likely to have more related pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "SDFQNmnfmD1L",
    "outputId": "7749a0d1-f6df-4ddb-cda7-c30a140a3ee5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "      <th>Related Count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <th>Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P/NP</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.241470</td>\n",
       "      <td>-0.200423</td>\n",
       "      <td>-0.380390</td>\n",
       "      <td>-0.153340</td>\n",
       "      <td>-0.189992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syn Count</th>\n",
       "      <td>-0.241470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.595940</td>\n",
       "      <td>0.653986</td>\n",
       "      <td>0.405877</td>\n",
       "      <td>0.801382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Related Count</th>\n",
       "      <td>-0.200423</td>\n",
       "      <td>0.595940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.598271</td>\n",
       "      <td>0.759510</td>\n",
       "      <td>0.373280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length</th>\n",
       "      <td>-0.380390</td>\n",
       "      <td>0.653986</td>\n",
       "      <td>0.598271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315872</td>\n",
       "      <td>0.345036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <td>-0.153340</td>\n",
       "      <td>0.405877</td>\n",
       "      <td>0.759510</td>\n",
       "      <td>0.315872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syn Count / Len</th>\n",
       "      <td>-0.189992</td>\n",
       "      <td>0.801382</td>\n",
       "      <td>0.373280</td>\n",
       "      <td>0.345036</td>\n",
       "      <td>0.428368</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     P/NP  Syn Count  Related Count    Length  \\\n",
       "P/NP             1.000000  -0.241470      -0.200423 -0.380390   \n",
       "Syn Count       -0.241470   1.000000       0.595940  0.653986   \n",
       "Related Count   -0.200423   0.595940       1.000000  0.598271   \n",
       "Length          -0.380390   0.653986       0.598271  1.000000   \n",
       "Rel Count / Len -0.153340   0.405877       0.759510  0.315872   \n",
       "Syn Count / Len -0.189992   0.801382       0.373280  0.345036   \n",
       "\n",
       "                 Rel Count / Len  Syn Count / Len  \n",
       "P/NP                   -0.153340        -0.189992  \n",
       "Syn Count               0.405877         0.801382  \n",
       "Related Count           0.759510         0.373280  \n",
       "Length                  0.315872         0.345036  \n",
       "Rel Count / Len         1.000000         0.428368  \n",
       "Syn Count / Len         0.428368         1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "BKXuGCUmZuTG",
    "outputId": "6ee66620-02b3-4589-b50d-95eb1df29c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Related count / len for Puns is 1.2443563344509985\n",
      "Mean Related count / len for Non-Puns is 0.6880516066453194\n",
      "Mean Syn count / len for Puns is 0.06401503045981599\n",
      "Mean Syn count / len for Non-Puns is 0.03089545164488603\n"
     ]
    }
   ],
   "source": [
    "sum_pun = df.loc[df['P/NP'] == 0]['Rel Count / Len'].sum()\n",
    "len_pun = len(df.loc[df['P/NP'] == 0]['Rel Count / Len'])\n",
    "mean_pun = sum_pun / len_pun\n",
    "print('Mean Related count / len for Puns is', mean_pun)\n",
    "\n",
    "sum_non_pun = df.loc[df['P/NP'] == 1]['Rel Count / Len'].sum()\n",
    "len_non_pun = len(df.loc[df['P/NP'] == 1]['Rel Count / Len'])\n",
    "mean_non_pun = sum_non_pun / len_non_pun\n",
    "print('Mean Related count / len for Non-Puns is', mean_non_pun)\n",
    "\n",
    "sum_pun = df.loc[df['P/NP'] == 0]['Syn Count / Len'].sum()\n",
    "len_pun = len(df.loc[df['P/NP'] == 0]['Syn Count / Len'])\n",
    "mean_pun = sum_pun / len_pun\n",
    "print('Mean Syn count / len for Puns is', mean_pun)\n",
    "\n",
    "sum_non_pun = df.loc[df['P/NP'] == 1]['Syn Count / Len'].sum()\n",
    "len_non_pun = len(df.loc[df['P/NP'] == 1]['Syn Count / Len'])\n",
    "mean_non_pun = sum_non_pun / len_non_pun\n",
    "print('Mean Syn count / len for Non-Puns is', mean_non_pun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWL8jgk9niVV"
   },
   "source": [
    "We'll try to turn this correlation into an actionable \"algorithm\" to predict if a sentence is a pun or not. \n",
    "\n",
    "The following is another data set with 60 puns and 100 non-puns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "RmFYXOdxLX6g",
    "outputId": "d1130e8a-9df6-418c-c13e-d6de85cf38f5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The energizer bunny went to jail. He was charg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>I had only one superstition. I made sure to to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Our nation is too different, too diverse to sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Like sex in Victorian England, the reality of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nope. Unintended.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP\n",
       "31   The energizer bunny went to jail. He was charg...     0\n",
       "70   I had only one superstition. I made sure to to...     1\n",
       "78   Our nation is too different, too diverse to sa...     1\n",
       "132  Like sex in Victorian England, the reality of ...     1\n",
       "2                                    Nope. Unintended.     0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('puns_test.csv')\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYPxIGisoGaJ"
   },
   "source": [
    "Let's now code the \"algorithm\".\n",
    "\n",
    "To do so, we need to know the threshold that we will use to determine if a sentence is a pun or not, based on the related pair count. \n",
    "\n",
    "Let's try using the mean Rel Count / Len first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "o95k48CcujNY",
    "outputId": "c3ef3e36-011f-45ed-b727-198bfaca085d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Predicted by Rel Count / Len</th>\n",
       "      <th>Predicted by Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>My parents were the same in the pulpit as they...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>World War II ended the Great Depression with o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What do you do with a dead chemist? You barium.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What should you do if you’re cold? Stand in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Buddhist walks up to a hot dog stand and say...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  \\\n",
       "113  My parents were the same in the pulpit as they...     1   \n",
       "145  World War II ended the Great Depression with o...     1   \n",
       "11     What do you do with a dead chemist? You barium.     0   \n",
       "29   What should you do if you’re cold? Stand in th...     0   \n",
       "5    A Buddhist walks up to a hot dog stand and say...     0   \n",
       "\n",
       "     Predicted by Rel Count / Len  Predicted by Syn Count / Len  \n",
       "113                             1                             1  \n",
       "145                             1                             1  \n",
       "11                              0                             0  \n",
       "29                              0                             0  \n",
       "5                               0                             1  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rel_count_predict(s, threshold=0.6880516066453194):\n",
    "    rel_count = common_related(s)\n",
    "    rel_count_len = rel_count / num_words(s)\n",
    "    if rel_count_len <= threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def syn_count_predict(s, threshold=0.03089545164488603):\n",
    "    syn_count = common_syn(s)\n",
    "    syn_count_len = syn_count / num_words(s)\n",
    "    if syn_count_len <= threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "test_df['Predicted by Rel Count / Len'] = test_df['Sentence'].apply(rel_count_predict)\n",
    "test_df['Predicted by Syn Count / Len'] = test_df['Sentence'].apply(syn_count_predict)\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for Synonym Count / Len is\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.74      0.59        58\n",
      "          1       0.79      0.56      0.65       100\n",
      "\n",
      "avg / total       0.68      0.63      0.63       158\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = test_df['P/NP']\n",
    "y_pred = test_df['Predicted by Syn Count / Len']\n",
    "conmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(conmat)\n",
    "sns.set()\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "\n",
    "print('The classification report for Synonym Count / Len is')\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "4be0J-u0q6sL",
    "outputId": "3df86d89-cb93-4f66-d89c-b16d26e8b3fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for Related Count / Len is\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.79      0.62        58\n",
      "          1       0.82      0.55      0.66       100\n",
      "\n",
      "avg / total       0.71      0.64      0.64       158\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = test_df['P/NP']\n",
    "y_pred = test_df['Predicted by Rel Count / Len']\n",
    "conmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(conmat)\n",
    "sns.set()\n",
    "\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "\n",
    "print('The classification report for Related Count / Len is')\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Predicted by Rel Count / Len</th>\n",
       "      <th>Predicted by Syn Count / Len</th>\n",
       "      <th>Predicted by Rel and Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>In matters of conscience, first thoughts are b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>There is a great difference between Christiani...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The truth is, hardly any of us have ethical en...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>I'm no longer dependent on the movie business ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>I had only one superstition. I made sure to to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  \\\n",
       "115  In matters of conscience, first thoughts are b...     1   \n",
       "141  There is a great difference between Christiani...     1   \n",
       "72   The truth is, hardly any of us have ethical en...     1   \n",
       "124  I'm no longer dependent on the movie business ...     1   \n",
       "70   I had only one superstition. I made sure to to...     1   \n",
       "\n",
       "     Predicted by Rel Count / Len  Predicted by Syn Count / Len  \\\n",
       "115                             1                             1   \n",
       "141                             0                             1   \n",
       "72                              0                             1   \n",
       "124                             1                             1   \n",
       "70                              1                             1   \n",
       "\n",
       "     Predicted by Rel and Syn Count / Len  \n",
       "115                                     1  \n",
       "141                                     1  \n",
       "72                                      1  \n",
       "124                                     1  \n",
       "70                                      1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rel_and_syn_count_predict(s, threshold1=0.6880516066453194, threshold2=0.03089545164488603):\n",
    "    rel_count = common_related(s)\n",
    "    rel_count_len = rel_count / num_words(s)\n",
    "    syn_count = common_syn(s)\n",
    "    syn_count_len = syn_count / num_words(s)\n",
    "    \n",
    "    if rel_count_len <= threshold1 and syn_count_len <= threshold2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "test_df['Predicted by Rel and Syn Count / Len'] = test_df['Sentence'].apply(rel_and_syn_count_predict)\n",
    "\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for Related and Synonym Count / Len is\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.66      0.58        58\n",
      "          1       0.77      0.66      0.71       100\n",
      "\n",
      "avg / total       0.68      0.66      0.66       158\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = test_df['P/NP']\n",
    "y_pred = test_df['Predicted by Rel and Syn Count / Len']\n",
    "conmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(conmat)\n",
    "sns.set()\n",
    "\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "\n",
    "print('The classification report for Related and Synonym Count / Len is')\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some testing, it seems the most accurate result we can get comes when the threshold is set such that \n",
    "\n",
    "---\n",
    "\n",
    "Original Hypothesis: Puns will have more related pairs of words than Non-Puns\n",
    "Finding: In reality, it seems that the more related pairs of words a sentence has, the more likely that it is **not** a pun.\n",
    "\n",
    "The idea behind this might be the fact that "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Converging meanings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
