{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tfortammi/pun-detector/blob/master/Converging_meanings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRUBsfYZJDu2"
   },
   "source": [
    "# Dappity Dap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmCZLMFXK__x"
   },
   "source": [
    "### Characteristics of Puns\n",
    "* Converging Meanings \n",
    "* Sound \n",
    "* Association\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lriPsNERLdhM"
   },
   "source": [
    "### Target: Converging Meanings\n",
    "\n",
    "We have observed that puns often make use of words that have very similar meanings. For example:\n",
    "\n",
    "'He said I was **average** - but he was just being **mean**.'\n",
    "\n",
    "where 'average' and 'mean' have the same meanings but are expressed differently. \n",
    "\n",
    "___\n",
    "\n",
    "In order to test this, we will do the following:\n",
    "\n",
    "* Step 1: Use Synset to list synonyms of tokens\n",
    "* Step 2: Find common words in Synsets within a sentence\n",
    "* Step 3: Determine correlation between converging meanings & whether a sentence is a pun or not\n",
    "\n",
    "---\n",
    "\n",
    "Import/Download relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "a2yw0bHqtbkE",
    "outputId": "62735e92-b2e3-4ae2-b1e3-fc6e02dd1405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tammi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCfLoO_cNw19"
   },
   "source": [
    "For this method, we will use NLTK's WordNet corpus to find the synsets of each token in a sentence.\n",
    "\n",
    "As an example, let's test it out on the word **'plant'** first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "pK56tdcLLVxl",
    "outputId": "fa163812-56cd-4a2e-9070-0636212d26e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Case  0\n",
      "Synset('plant.n.01')\n",
      "buildings for carrying on industrial labor\n",
      "['plant', 'works', 'industrial_plant']\n",
      " \n",
      "Use Case  1\n",
      "Synset('plant.n.02')\n",
      "(botany) a living organism lacking the power of locomotion\n",
      "['plant', 'flora', 'plant_life']\n",
      " \n",
      "Use Case  2\n",
      "Synset('plant.n.03')\n",
      "an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
      "['plant']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "word = Word('plant')\n",
    "for i in range(3):\n",
    "    print('Use Case ', i)\n",
    "    print(word.synsets[i])\n",
    "    print(word.definitions[i])\n",
    "    print(word.synsets[i].lemma_names())\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hvCZaYRvymW"
   },
   "source": [
    "Through WordNet, the **use cases** (Synsets) of the word \"Plant\" can be found, as well as the **definitions** and **Synonyms** (Lemma Names) as the input.\n",
    "\n",
    "---\n",
    "        \n",
    "           \n",
    "Let's first eyeball how relevant the lemmas of each significant word in a sentence to determining if a sentence is a pun. \n",
    "\n",
    "**The example we will use is: \"The past, the present and the future walked into a bar. It was tense.\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "aMjxEEmJOyLw",
    "outputId": "5983e570-99f4-4ea8-dfda-912dcd714c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tammi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tammi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# First, importing relevant packages, etc\n",
    "\n",
    "import codecs\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxir3NmAx4Zf"
   },
   "source": [
    "We'll need to process the sentence, which includes lemmatizing, filtering out stop words, stripping punctuation and tokenizing the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1491
    },
    "colab_type": "code",
    "id": "FTK9plVwOSiN",
    "outputId": "8a74d808-8264-4f19-923c-23bfa8800f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered word: 'The' and its lemmas:\n",
      "\n",
      "Filtered word: 'past' and its lemmas:\n",
      "['past', 'past_times', 'yesteryear']\n",
      "['past']\n",
      "['past', 'past_tense']\n",
      "['past']\n",
      "['past', 'preceding', 'retiring']\n",
      "['by', 'past']\n",
      "\n",
      "Filtered word: 'present' and its lemmas:\n",
      "['present', 'nowadays']\n",
      "['present']\n",
      "['present', 'present_tense']\n",
      "['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "['present', 'represent', 'lay_out']\n",
      "['stage', 'present', 'represent']\n",
      "['present', 'submit']\n",
      "['present', 'pose']\n",
      "['award', 'present']\n",
      "['give', 'gift', 'present']\n",
      "['deliver', 'present']\n",
      "['introduce', 'present', 'acquaint']\n",
      "['portray', 'present']\n",
      "['confront', 'face', 'present']\n",
      "['present']\n",
      "['salute', 'present']\n",
      "['present']\n",
      "['present']\n",
      "\n",
      "Filtered word: 'future' and its lemmas:\n",
      "['future', 'hereafter', 'futurity', 'time_to_come']\n",
      "['future', 'future_tense']\n",
      "['future']\n",
      "['future']\n",
      "['future']\n",
      "['future', 'next', 'succeeding']\n",
      "['future']\n",
      "\n",
      "Filtered word: 'walked' and its lemmas:\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk']\n",
      "['walk', 'take_the_air']\n",
      "\n",
      "Filtered word: 'bar' and its lemmas:\n",
      "['barroom', 'bar', 'saloon', 'ginmill', 'taproom']\n",
      "['bar']\n",
      "['bar']\n",
      "['measure', 'bar']\n",
      "['bar']\n",
      "['prevention', 'bar']\n",
      "['bar']\n",
      "['bar']\n",
      "['legal_profession', 'bar', 'legal_community']\n",
      "['stripe', 'streak', 'bar']\n",
      "['cake', 'bar']\n",
      "['Browning_automatic_rifle', 'BAR']\n",
      "['bar']\n",
      "['bar']\n",
      "['bar']\n",
      "['bar', 'debar', 'exclude']\n",
      "['barricade', 'block', 'blockade', 'stop', 'block_off', 'block_up', 'bar']\n",
      "['banish', 'relegate', 'bar']\n",
      "['bar']\n",
      "\n",
      "Filtered word: 'It' and its lemmas:\n",
      "['information_technology', 'IT']\n",
      "\n",
      "Filtered word: 'tense' and its lemmas:\n",
      "['tense']\n",
      "['strain', 'tense']\n",
      "['tense']\n",
      "['tense', 'tense_up']\n",
      "['tense', 'strain', 'tense_up']\n",
      "['tense']\n",
      "['tense']\n",
      "['tense']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simpleFilter(sentence):\n",
    "    \n",
    "    '''This function filters out stopwords, lemmatizes, tokenizes, and \n",
    "    strips punctuation from the input sentence and returns the a list of \n",
    "    filtered tokens'''\n",
    "    \n",
    "    filtered_sent = []\n",
    "    \n",
    "    # Strip punctuation\n",
    "    stripped = re.sub(\"[(.)',=?!#@]\", '', sentence)\n",
    "        \n",
    "    # filter out stopwords \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(stripped)\n",
    "    \n",
    "    # Lemmatize and Filter out Stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    return filtered_sent\n",
    "  \n",
    "def printLemmas(word):\n",
    "    \n",
    "    '''This function prints out all synonyms of a given word.'''\n",
    "    \n",
    "    for ss in Word(word).synsets:\n",
    "        print(ss.lemma_names())\n",
    "        \n",
    "\n",
    "# Print \n",
    "\n",
    "s = 'The past, the present and the future walked into a bar. It was tense.'\n",
    "\n",
    "for word in simpleFilter(s):\n",
    "    print(\"Filtered word: '\" + word + \"' and its lemmas:\")\n",
    "    printLemmas(word)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxl_W9dEvRWZ"
   },
   "source": [
    "---\n",
    "## **Hypothesis 1: Converging Meaning Pun**\n",
    "\n",
    "We observe that the word 'tense' appears as a synonym of the words 'present', 'past', and 'future'. Since we are exploring puns with converging meanings, **we hypothesise that we are more likely to find words with converging meanings in puns than in non-puns.**\n",
    "\n",
    "---\n",
    "\n",
    "To do this, we first produce a list of unique synonyms of a certain word, excluding the word itself.\n",
    "\n",
    "\n",
    "Let's try this on the word \"plant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "GZrNVNuoQmdK",
    "outputId": "3efd52fa-e560-4e15-a9a3-fdbe57206cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'industrial_plant', 'flora', 'plant_life', 'set', 'implant', 'engraft', 'embed', 'imbed', 'establish', 'found', 'constitute', 'institute']\n"
     ]
    }
   ],
   "source": [
    "def create_lemmas(word):\n",
    "    lemmas_list = []\n",
    "    for ss in Word(word).synsets:\n",
    "        lemmas_list.append(ss.lemma_names())\n",
    "    return lemmas_list\n",
    "\n",
    "def process_lemmas(lemmas_list, word):\n",
    "    '''\n",
    "    This function process the lemma list of all the definition of a word\n",
    "    and returns a list of all associated unrepeated words with the word\n",
    "    '''\n",
    "    all_lemmas = []\n",
    "    for each_list in lemmas_list:\n",
    "        for lemma in each_list:\n",
    "            if lemma != word and lemma not in all_lemmas:\n",
    "                all_lemmas.append(lemma)\n",
    "    return all_lemmas\n",
    "\n",
    "\n",
    "print(process_lemmas(create_lemmas('plant'), 'plant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7PhQGRD4XEU"
   },
   "source": [
    "Next, we have to find out if synonyms of any word in a sentence can be found in the rest of the sentence, and count the number of times this occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "coVT022UOrwf",
    "outputId": "aa80c0be-b964-4b6b-cc51-745cb40e3300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past tense\n",
      "present tense\n",
      "future tense\n",
      "The number of synonym pairs in this sentence is 3\n"
     ]
    }
   ],
   "source": [
    "def print_common_syn(s):\n",
    "    \n",
    "    '''\n",
    "    This function takes in a sentence, processes and tokenizes it and\n",
    "    prints each significant word and tests if its synonyms can be found\n",
    "    in the rest of the sentence. It prints the pair and returns the\n",
    "    number of pairs found.\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # Filter the sentence to remove filler words / stopwords\n",
    "    filtered_words = simpleFilter(s)\n",
    "    \n",
    "    for index, word in enumerate(filtered_words):\n",
    "        if word.isalpha():\n",
    "            lemma_list_of_term = process_lemmas(create_lemmas(word),word)\n",
    "\n",
    "            # test if any word in the rest of the sentence appears in the lemma list of current word\n",
    "            for other_word in filtered_words[index+1:]:\n",
    "                if other_word in ' '.join(lemma_list_of_term):\n",
    "                    count += 1\n",
    "                    print(word, other_word)\n",
    "    return count\n",
    "    \n",
    "    \n",
    "s = 'The past, the present and the future walked into a bar. It was tense.'\n",
    "print('The number of synonym pairs in this sentence is',print_common_syn(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_syn(s):\n",
    "    \n",
    "    '''\n",
    "    This function takes in a sentence, processes and tokenizes it and\n",
    "    prints each significant word and tests if its synonyms can be found\n",
    "    in the rest of the sentence. It prints the pair and returns the\n",
    "    number of pairs found.\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # Filter the sentence to remove filler words / stopwords\n",
    "    filtered_words = simpleFilter(s)\n",
    "    \n",
    "    for index, word in enumerate(filtered_words):\n",
    "        if word.isalpha():\n",
    "            lemma_list_of_term = process_lemmas(create_lemmas(word),word)\n",
    "\n",
    "            # test if any word in the rest of the sentence appears in the lemma list of current word\n",
    "            for other_word in filtered_words[index+1:]:\n",
    "                if other_word in ' '.join(lemma_list_of_term):\n",
    "                    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rXRmeVvyfbo"
   },
   "source": [
    "In order to see if this method does work, we will test it out on our list of pre-tagged puns and non-puns where puns are tagged '0' and non-puns are tagged '1'\n",
    "\n",
    "We import the list and apply our function common_syn to it, under the label 'Syn Count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10789
    },
    "colab_type": "code",
    "id": "SDLtb-6fXKWr",
    "outputId": "e65b603b-3091-4da6-c8f8-2d508ec9dda7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Life is a culmination of the past an awareness...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Whenever you find yourself on the side of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>What do you call crystal clear urine? 1080pee.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>I would rather be a superb meteor, every atom ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>The first to apologize is the bravest. The fir...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  Syn Count\n",
       "475  Life is a culmination of the past an awareness...     0          1\n",
       "329  Whenever you find yourself on the side of the ...     0          0\n",
       "147     What do you call crystal clear urine? 1080pee.     1          0\n",
       "260  I would rather be a superb meteor, every atom ...     0          1\n",
       "272  The first to apologize is the bravest. The fir...     0          3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('puns_final.csv', encoding='latin-1')\n",
    "df['Syn Count'] = df['Sentence'].apply(common_syn)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKVUEdxK6b1o"
   },
   "source": [
    "To find out if this method is accurate, we use the correlation between whether the sentence is a pun or not and the Syn Count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "laC4Fb4QYmF2",
    "outputId": "427986ae-0468-434a-cf93-f0e698a16aea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P/NP</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.24147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syn Count</th>\n",
       "      <td>-0.24147</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P/NP  Syn Count\n",
       "P/NP       1.00000   -0.24147\n",
       "Syn Count -0.24147    1.00000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-9IxQXp6pIP"
   },
   "source": [
    "In this case, it appears the Syn Count is not very highly correlated with whether the sentence is a pun or not...\n",
    "\n",
    "Perhaps we should try a different approach.\n",
    "\n",
    "---\n",
    "\n",
    "Other than the ability to find synonyms, WordNet can also find out a range of other details about a word.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crXdzXjP-HKv"
   },
   "source": [
    "Let's use the words 'happy' and 'cutlery' to see what kind of details WordNet can figure out about a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "colab_type": "code",
    "id": "XBq094Cc8U0W",
    "outputId": "2b70f51f-9a28-43f2-bb15-e9073f2b613f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are synonyms of 'happy':\n",
      "('happy', 'happy.a.01')\n",
      "('felicitous', 'felicitous.s.02')\n",
      "('happy', 'felicitous.s.02')\n",
      "('glad', 'glad.s.02')\n",
      "('happy', 'glad.s.02')\n",
      "('happy', 'happy.s.04')\n",
      "('well-chosen', 'happy.s.04')\n",
      "\n",
      "The following are hyponyms (words that are more specific) of 'cutlery':\n",
      "('bolt_cutter', 'bolt_cutter.n.01')\n",
      "('cigar_cutter', 'cigar_cutter.n.01')\n",
      "('die', 'die.n.03')\n",
      "('edge_tool', 'edge_tool.n.01')\n",
      "('glass_cutter', 'glass_cutter.n.03')\n",
      "('tile_cutter', 'tile_cutter.n.01')\n",
      "('fork', 'fork.n.01')\n",
      "('spoon', 'spoon.n.01')\n",
      "('Spork', 'spork.n.01')\n",
      "('table_knife', 'table_knife.n.01')\n",
      "\n",
      "The following are similar to 'happy':\n",
      "('blessed', 'blessed.s.06')\n",
      "('blissful', 'blissful.s.01')\n",
      "('bright', 'bright.s.09')\n",
      "('golden', 'golden.s.02')\n",
      "('halcyon', 'golden.s.02')\n",
      "('prosperous', 'golden.s.02')\n",
      "('laughing', 'laughing.s.01')\n",
      "('riant', 'laughing.s.01')\n",
      "('fortunate', 'fortunate.a.01')\n",
      "('willing', 'willing.a.01')\n",
      "('felicitous', 'felicitous.a.01')\n",
      "\n",
      "The following are antonyms (opposite) of 'happy':\n",
      "('unhappy', 'unhappy.a.01')\n",
      "\n",
      "The following are words that should also be seen with 'happy':\n",
      "('cheerful', 'cheerful.a.01')\n",
      "('contented', 'contented.a.01')\n",
      "('content', 'contented.a.01')\n",
      "('elated', 'elated.a.01')\n",
      "('euphoric', 'euphoric.a.01')\n",
      "('felicitous', 'felicitous.a.01')\n",
      "('glad', 'glad.a.01')\n",
      "('joyful', 'joyful.a.01')\n",
      "('joyous', 'joyous.a.01')\n"
     ]
    }
   ],
   "source": [
    "print(\"The following are synonyms of 'happy':\")\n",
    "for x in get_all_synsets('happy'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are hyponyms (words that are more specific) of 'cutlery':\")\n",
    "for x in get_all_hyponyms('cutlery'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are similar to 'happy':\")\n",
    "for x in get_all_similar_tos('happy'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are antonyms (opposite) of 'happy':\")\n",
    "for x in get_all_antonyms('happy'):\n",
    "    print(x)\n",
    "print()\n",
    "print(\"The following are words that should also be seen with 'happy':\")\n",
    "for x in get_all_also_sees('happy'):\n",
    "    print(x)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxCPM76im7dv"
   },
   "source": [
    "The functions below make use of WordNet to yield synonyms, hyponyms, antonyms, words that are similar to as well as words that the WordNet corpus has recorded as \"also sees\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnkjHc1Kb6aI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_all_synsets(word, pos=None):\n",
    "    for ss in wn.synsets(word):\n",
    "        for lemma in ss.lemma_names():\n",
    "            yield (lemma, ss.name())\n",
    "\n",
    "\n",
    "def get_all_hyponyms(word, pos=None):\n",
    "    for ss in wn.synsets(word, pos=pos):\n",
    "            for hyp in ss.hyponyms():\n",
    "                for lemma in hyp.lemma_names():\n",
    "                    yield (lemma, hyp.name())\n",
    "\n",
    "\n",
    "def get_all_similar_tos(word, pos=None):\n",
    "    for ss in wn.synsets(word):\n",
    "            for sim in ss.similar_tos():\n",
    "                for lemma in sim.lemma_names():\n",
    "                    yield (lemma, sim.name())\n",
    "\n",
    "\n",
    "def get_all_antonyms(word, pos=None):\n",
    "    for ss in wn.synsets(word, pos=None):\n",
    "        for sslema in ss.lemmas():\n",
    "            for antlemma in sslema.antonyms():\n",
    "                    yield (antlemma.name(), antlemma.synset().name())\n",
    "\n",
    "\n",
    "def get_all_also_sees(word, pos=None):\n",
    "        for ss in wn.synsets(word):\n",
    "            for also in ss.also_sees():\n",
    "                for lemma in also.lemma_names():\n",
    "                    yield (lemma, also.name())\n",
    "\n",
    "\n",
    "def get_all_synonyms(word, pos=None):\n",
    "    for x in get_all_synsets(word, pos):\n",
    "        yield (x[0], x[1], 'ss')\n",
    "    for x in get_all_hyponyms(word, pos):\n",
    "        yield (x[0], x[1], 'hyp')\n",
    "    for x in get_all_similar_tos(word, pos):\n",
    "        yield (x[0], x[1], 'sim')\n",
    "    for x in get_all_antonyms(word, pos):\n",
    "        yield (x[0], x[1], 'ant')\n",
    "    for x in get_all_also_sees(word, pos):\n",
    "        yield (x[0], x[1], 'also')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2mBXMIL-R8z"
   },
   "source": [
    "Let's all the categories above words that are **related** to the main word. \n",
    "\n",
    "Now, we want to do the same as we did for the synonym count and define some functions that will find the common related words - not just within the sentence, but also with the related words of the other words in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0snqjLX-lKD-"
   },
   "source": [
    "**Example:**\n",
    "\n",
    "'What do you call a belt with a watch on it? A waist of time.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3wnzCQ5gRjO"
   },
   "outputs": [],
   "source": [
    "def related_list(word):\n",
    "    lemma_list = []\n",
    "    for x in get_all_synonyms(word):\n",
    "        lemma_list.append(x)\n",
    "        for y in get_all_synonyms(x[0]):\n",
    "            lemma_list.append(y)\n",
    "    return list(set(lemma_list))\n",
    "\n",
    "def common_related(s):\n",
    "    filtered = simpleFilter(s)\n",
    "    count = 0\n",
    "    for index, word in enumerate(filtered):\n",
    "        related = related_list(word)\n",
    "        for r_set in related:\n",
    "            if r_set[0] in filtered[index+1:]:\n",
    "                count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "h17Biokqk0QQ",
    "outputId": "6c996f20-fbd4-432b-f715-4d6b484717d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: What do you call a belt with a watch on it? A waist of time.\n",
      "['What', 'call', 'belt', 'watch', 'A', 'waist', 'time']\n",
      "--------------------------------------------------\n",
      "\n",
      "The word 'call' in the sentence is related to 'watch' as ('watch', 'watch.v.03', 'ss') to mean 'see or watch'\n",
      "\n",
      "The word 'call' in the sentence is related to 'time' as ('time', 'prison_term.n.01', 'hyp') to mean 'the period of time a prisoner is imprisoned'\n",
      "\n",
      "The word 'call' in the sentence is related to 'watch' as ('watch', 'determine.v.08', 'ss') to mean 'find out, learn, or determine with certainty, usually by making an inquiry or other effort'\n",
      "\n",
      "The word 'watch' in the sentence is related to 'time' as ('time', 'time.v.03', 'hyp') to mean 'set the speed, duration, or execution of'\n",
      "\n",
      "--------------------------------------------------\n",
      "Number of Related pairs: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = 'What do you call a belt with a watch on it? A waist of time.'\n",
    "\n",
    "filtered = simpleFilter(s)\n",
    "count = 0\n",
    "print('Sentence:',s)\n",
    "print(filtered)\n",
    "print('-----' *10)\n",
    "print()\n",
    "for index, word in enumerate(filtered):\n",
    "    related = related_list(word)\n",
    "    for r_set in related:\n",
    "        if r_set[0] in filtered[index+1:]:\n",
    "            print(\"The word '\" + word + \"' in the sentence is related to '\" + r_set[0] + \"' as\", r_set, \"to mean '\" + wordnet.synset(r_set[1]).definition() +\"'\")\n",
    "            print()\n",
    "            \n",
    "            count += 1\n",
    "print('-----' * 10)\n",
    "print('Number of Related pairs:', count)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3k1fliJg2XnI"
   },
   "source": [
    "Now we want to apply this to the rest of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "T9mz_g5plIsT",
    "outputId": "79a36c2e-5ee6-4811-d4fb-06571a60d943"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "      <th>Related Count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <th>Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Architecture is the art of how to waste space</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Why does the singer of Cheap Thrills not want ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Seventy percent of success in life is showing up</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>I was accused of being a plagiarist, their wor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>The greatest thing in the world is to know how...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  Syn Count  \\\n",
       "478      Architecture is the art of how to waste space     0          0   \n",
       "47   Why does the singer of Cheap Thrills not want ...     1          3   \n",
       "487   Seventy percent of success in life is showing up     0          0   \n",
       "50   I was accused of being a plagiarist, their wor...     1          0   \n",
       "264  The greatest thing in the world is to know how...     0          0   \n",
       "\n",
       "     Related Count  Length  Rel Count / Len  Syn Count / Len  \n",
       "478              0       9         0.000000             0.00  \n",
       "47               0      12         0.000000             0.25  \n",
       "487              0       9         0.000000             0.00  \n",
       "50               0      11         0.000000             0.00  \n",
       "264              1      14         0.071429             0.00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_words(s):\n",
    "    sentence = s.split()\n",
    "    num_words = len(sentence)\n",
    "    return num_words\n",
    "\n",
    "df['Related Count'] = df['Sentence'].apply(common_related)\n",
    "df['Length'] = df['Sentence'].apply(num_words)\n",
    "df['Rel Count / Len'] = df['Related Count'] / df['Length']\n",
    "df['Syn Count / Len'] = df['Syn Count'] / df['Length']\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uldmmxd4hc_N"
   },
   "source": [
    "Here is a description of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "QQS8AyWghNb9",
    "outputId": "e62f55c3-8e42-4b8a-b1c0-13f8749c5153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "      <th>Related Count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <th>Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>521.00000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.49904</td>\n",
       "      <td>0.978887</td>\n",
       "      <td>19.583493</td>\n",
       "      <td>15.053743</td>\n",
       "      <td>0.966738</td>\n",
       "      <td>0.047487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50048</td>\n",
       "      <td>2.506441</td>\n",
       "      <td>57.222115</td>\n",
       "      <td>8.787939</td>\n",
       "      <td>1.815702</td>\n",
       "      <td>0.087244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.095238</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>813.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>0.548387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            P/NP   Syn Count  Related Count      Length  Rel Count / Len  \\\n",
       "count  521.00000  521.000000     521.000000  521.000000       521.000000   \n",
       "mean     0.49904    0.978887      19.583493   15.053743         0.966738   \n",
       "std      0.50048    2.506441      57.222115    8.787939         1.815702   \n",
       "min      0.00000    0.000000       0.000000    5.000000         0.000000   \n",
       "25%      0.00000    0.000000       0.000000   10.000000         0.000000   \n",
       "50%      0.00000    0.000000       3.000000   13.000000         0.230769   \n",
       "75%      1.00000    1.000000      17.000000   16.000000         1.095238   \n",
       "max      1.00000   32.000000     813.000000   74.000000        15.625000   \n",
       "\n",
       "       Syn Count / Len  \n",
       "count       521.000000  \n",
       "mean          0.047487  \n",
       "std           0.087244  \n",
       "min           0.000000  \n",
       "25%           0.000000  \n",
       "50%           0.000000  \n",
       "75%           0.071429  \n",
       "max           0.548387  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9sgFr4Mgi1K"
   },
   "source": [
    "The code below finds the correlation between the different variables in the data frame. \n",
    "\n",
    "As can be seen, the correlation between whether a sentence is a pun or not and the number of related count pairs is debatable.\n",
    "\n",
    "We also took related count / len of sentence as a longer sentence is more likely to have more related pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "SDFQNmnfmD1L",
    "outputId": "7749a0d1-f6df-4ddb-cda7-c30a140a3ee5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Syn Count</th>\n",
       "      <th>Related Count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <th>Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P/NP</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.241470</td>\n",
       "      <td>-0.200423</td>\n",
       "      <td>-0.380390</td>\n",
       "      <td>-0.153340</td>\n",
       "      <td>-0.189992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syn Count</th>\n",
       "      <td>-0.241470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.595940</td>\n",
       "      <td>0.653986</td>\n",
       "      <td>0.405877</td>\n",
       "      <td>0.801382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Related Count</th>\n",
       "      <td>-0.200423</td>\n",
       "      <td>0.595940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.598271</td>\n",
       "      <td>0.759510</td>\n",
       "      <td>0.373280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length</th>\n",
       "      <td>-0.380390</td>\n",
       "      <td>0.653986</td>\n",
       "      <td>0.598271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315872</td>\n",
       "      <td>0.345036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rel Count / Len</th>\n",
       "      <td>-0.153340</td>\n",
       "      <td>0.405877</td>\n",
       "      <td>0.759510</td>\n",
       "      <td>0.315872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syn Count / Len</th>\n",
       "      <td>-0.189992</td>\n",
       "      <td>0.801382</td>\n",
       "      <td>0.373280</td>\n",
       "      <td>0.345036</td>\n",
       "      <td>0.428368</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     P/NP  Syn Count  Related Count    Length  \\\n",
       "P/NP             1.000000  -0.241470      -0.200423 -0.380390   \n",
       "Syn Count       -0.241470   1.000000       0.595940  0.653986   \n",
       "Related Count   -0.200423   0.595940       1.000000  0.598271   \n",
       "Length          -0.380390   0.653986       0.598271  1.000000   \n",
       "Rel Count / Len -0.153340   0.405877       0.759510  0.315872   \n",
       "Syn Count / Len -0.189992   0.801382       0.373280  0.345036   \n",
       "\n",
       "                 Rel Count / Len  Syn Count / Len  \n",
       "P/NP                   -0.153340        -0.189992  \n",
       "Syn Count               0.405877         0.801382  \n",
       "Related Count           0.759510         0.373280  \n",
       "Length                  0.315872         0.345036  \n",
       "Rel Count / Len         1.000000         0.428368  \n",
       "Syn Count / Len         0.428368         1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "BKXuGCUmZuTG",
    "outputId": "6ee66620-02b3-4589-b50d-95eb1df29c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Related count / len for Puns is 1.2443563344509985\n",
      "Mean Related count / len for Non-Puns is 0.6880516066453194\n",
      "Mean Syn count / len for Puns is 0.06401503045981599\n",
      "Mean Syn count / len for Non-Puns is 0.03089545164488603\n"
     ]
    }
   ],
   "source": [
    "sum_pun = df.loc[df['P/NP'] == 0]['Rel Count / Len'].sum()\n",
    "len_pun = len(df.loc[df['P/NP'] == 0]['Rel Count / Len'])\n",
    "mean_pun = sum_pun / len_pun\n",
    "print('Mean Related count / len for Puns is', mean_pun)\n",
    "\n",
    "sum_non_pun = df.loc[df['P/NP'] == 1]['Rel Count / Len'].sum()\n",
    "len_non_pun = len(df.loc[df['P/NP'] == 1]['Rel Count / Len'])\n",
    "mean_non_pun = sum_non_pun / len_non_pun\n",
    "print('Mean Related count / len for Non-Puns is', mean_non_pun)\n",
    "\n",
    "sum_pun = df.loc[df['P/NP'] == 0]['Syn Count / Len'].sum()\n",
    "len_pun = len(df.loc[df['P/NP'] == 0]['Syn Count / Len'])\n",
    "mean_pun = sum_pun / len_pun\n",
    "print('Mean Syn count / len for Puns is', mean_pun)\n",
    "\n",
    "sum_non_pun = df.loc[df['P/NP'] == 1]['Syn Count / Len'].sum()\n",
    "len_non_pun = len(df.loc[df['P/NP'] == 1]['Syn Count / Len'])\n",
    "mean_non_pun = sum_non_pun / len_non_pun\n",
    "print('Mean Syn count / len for Non-Puns is', mean_non_pun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWL8jgk9niVV"
   },
   "source": [
    "We'll try to turn this correlation into an actionable \"algorithm\" to predict if a sentence is a pun or not. \n",
    "\n",
    "The following is another data set with 60 puns and 100 non-puns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "RmFYXOdxLX6g",
    "outputId": "d1130e8a-9df6-418c-c13e-d6de85cf38f5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>What do you call a line of rabbits marching ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>I remember one day sitting at the pool and sud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Why should you never trust a train? They have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Most people ask for happiness on condition. Ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Neither the life of an individual nor the hist...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP\n",
       "53   What do you call a line of rabbits marching ba...     1\n",
       "82   I remember one day sitting at the pool and sud...     0\n",
       "57   Why should you never trust a train? They have ...     1\n",
       "99   Most people ask for happiness on condition. Ha...     0\n",
       "102  Neither the life of an individual nor the hist...     0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('puns_test.csv')\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYPxIGisoGaJ"
   },
   "source": [
    "Let's now code the \"algorithm\".\n",
    "\n",
    "To do so, we need to know the threshold that we will use to determine if a sentence is a pun or not, based on the related pair count. \n",
    "\n",
    "Let's try using the mean Rel Count / Len first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "o95k48CcujNY",
    "outputId": "c3ef3e36-011f-45ed-b727-198bfaca085d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Predicted by Rel Count / Len</th>\n",
       "      <th>Predicted by Syn Count / Len</th>\n",
       "      <th>Predicted by Rel and Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I couldnt remember how to throw a boomerang b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>We are also in the process of defining how bes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Do you know sign language? You should learn it...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>To write with a broken pencil is pointless.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>I had my footballing heroes such as Bryan Robs...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  \\\n",
       "27   I couldnt remember how to throw a boomerang b...     1   \n",
       "104  We are also in the process of defining how bes...     0   \n",
       "16   Do you know sign language? You should learn it...     1   \n",
       "25         To write with a broken pencil is pointless.     1   \n",
       "109  I had my footballing heroes such as Bryan Robs...     0   \n",
       "\n",
       "     Predicted by Rel Count / Len  Predicted by Syn Count / Len  \\\n",
       "27                              1                             0   \n",
       "104                             0                             0   \n",
       "16                              1                             0   \n",
       "25                              1                             1   \n",
       "109                             1                             1   \n",
       "\n",
       "     Predicted by Rel and Syn Count / Len  \n",
       "27                                      1  \n",
       "104                                     0  \n",
       "16                                      1  \n",
       "25                                      1  \n",
       "109                                     1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rel_count_predict(s, threshold=0.966738):\n",
    "    rel_count = common_related(s)\n",
    "    rel_count_len = rel_count / num_words(s)\n",
    "    if rel_count_len >= threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def syn_count_predict(s, threshold=0.03089545164488603):\n",
    "    syn_count = common_syn(s)\n",
    "    syn_count_len = syn_count / num_words(s)\n",
    "    if syn_count_len >= threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "test_df['Predicted by Rel Count / Len'] = test_df['Sentence'].apply(rel_count_predict)\n",
    "test_df['Predicted by Syn Count / Len'] = test_df['Sentence'].apply(syn_count_predict)\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for Synonym Count / Len is\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.56      0.65       100\n",
      "          1       0.49      0.74      0.59        58\n",
      "\n",
      "avg / total       0.68      0.63      0.63       158\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = test_df['P/NP']\n",
    "y_pred = test_df['Predicted by Syn Count / Len']\n",
    "conmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(conmat)\n",
    "sns.set()\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "\n",
    "print('The classification report for Synonym Count / Len is')\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "4be0J-u0q6sL",
    "outputId": "3df86d89-cb93-4f66-d89c-b16d26e8b3fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for Related Count / Len is\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.51      0.64       100\n",
      "          1       0.51      0.86      0.64        58\n",
      "\n",
      "avg / total       0.73      0.64      0.64       158\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = test_df['P/NP']\n",
    "y_pred = test_df['Predicted by Rel Count / Len']\n",
    "conmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(conmat)\n",
    "sns.set()\n",
    "\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "\n",
    "print('The classification report for Related Count / Len is')\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>P/NP</th>\n",
       "      <th>Predicted by Rel Count / Len</th>\n",
       "      <th>Predicted by Syn Count / Len</th>\n",
       "      <th>Predicted by Rel and Syn Count / Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>A cabbage and celery walk into a bar and the c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>My parents were the same in the pulpit as they...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>After the accident, the juggler didnt have th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>I don't live with people, that's why my relati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>What do you call a fake noodle? An impasta.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  P/NP  \\\n",
       "55   A cabbage and celery walk into a bar and the c...     1   \n",
       "113  My parents were the same in the pulpit as they...     0   \n",
       "23   After the accident, the juggler didnt have th...     1   \n",
       "147  I don't live with people, that's why my relati...     0   \n",
       "42         What do you call a fake noodle? An impasta.     1   \n",
       "\n",
       "     Predicted by Rel Count / Len  Predicted by Syn Count / Len  \\\n",
       "55                              1                             0   \n",
       "113                             0                             0   \n",
       "23                              1                             1   \n",
       "147                             0                             0   \n",
       "42                              1                             1   \n",
       "\n",
       "     Predicted by Rel and Syn Count / Len  \n",
       "55                                      1  \n",
       "113                                     0  \n",
       "23                                      1  \n",
       "147                                     0  \n",
       "42                                      1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rel_and_syn_count_predict(s, threshold1=0.966738, threshold2=0.03089545164488603):\n",
    "    rel_count = common_related(s)\n",
    "    rel_count_len = rel_count / num_words(s)\n",
    "    syn_count = common_syn(s)\n",
    "    syn_count_len = syn_count / num_words(s)\n",
    "    \n",
    "    if rel_count_len >= threshold1 and syn_count_len >= threshold2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "test_df['Predicted by Rel and Syn Count / Len'] = test_df['Sentence'].apply(rel_and_syn_count_predict)\n",
    "\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for Related and Synonym Count / Len is\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.41      0.56       100\n",
      "          1       0.47      0.91      0.62        58\n",
      "\n",
      "avg / total       0.74      0.59      0.58       158\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAEBCAYAAADfMaYGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFABJREFUeJzt3Xt4VPWdx/HP5IZBnLLCDLewcRV3qxSQYtHYNamlDZEQJcC63CSCawFN2MaqpchNKw8BbKM2i25bocEiCgpUEQOVZWlrQql0FwqliMAgQUwGUxwQyGXm7B8+OxYimUwC5zdzfL/6nOfxnMyc8/3r0y/f85tzXJZlWQIA2CrBdAEA8EVE+AKAAYQvABhA+AKAAYQvABhA+AKAAYQvABhA+AKAAYQvABhA+AKAAYQvABhA+AKAAUl2XuzjSd+y83KIAx0X/9R0CYhRyV2vbtf3G48ftO1abWFr+AKAbUJB0xW0iPAF4ExWyHQFLSJ8AThTiPAFANtZdL4AYECwyXQFLSJ8ATgTN9wAwADGDgBgADfcAMB+3HADABPofAHAgGCj6QpaRPgCcCbGDgBgAGMHADCAzhcADKDzBQD7WSFuuAGA/eh8AcAAZr4AYAAP1gEAA+h8AcAAZr4AYAAPUwcAA+h8AcB+lsUNNwCwH50vABjAagcAMIDOFwAMYLUDABjA2AEADGDsAAAGEL4AYMAlGjvcfffdqqurU1LSp/H5+OOP6/3339ezzz6rpqYmFRQUaPz48RHPQ/gCcKZLcMPNsiz5fD5t2bIlHL41NTUqLi7WmjVrlJKSojFjxuimm25Snz59WjwX4QvAmS7B2OHgwYOSpMmTJ+vEiRO66667dPnll+vmm29W586dJUlDhw5VRUWFCgsLWzwX4QvAmaIYOwQCAQUCgWbH3W633G73OZ/LyMjQ7Nmz1djYqIkTJ+r222+Xx+MJf8br9WrXrl0Rr0n4AnCmKDrf8vJylZWVNTteWFiooqKi8P7AgQM1cODA8P7o0aO1YMECTZs2LXzMsiy5XK6I1yR8AThTFOFbUFCg/Pz8Zsf/tuuVpHfeeUeNjY3KyMiQ9GnQ9urVS36/P/wZv98vr9cb8ZoJra4OAOKJZbV6c7vdSktLa7adH74nT57UokWLVF9fr1OnTmnt2rVavHixqqqqVFdXpzNnzmjTpk3KzMyMWB6dLwBnarr4qx1uu+027dy5UyNGjFAoFNK4ceM0aNAgFRcXa+LEiWpsbNTo0aPVv3//iOdyWZZlXfQKL+DjSd+y61KIEx0X/9R0CYhRyV2vbtf3z/zy0VZ/NnXC/HZdqy3ofAE4E79wAwAD7PtHfZsQvgCcic4XAAwgfAHAflaQF2gCgP3ofAHAAN5kAQAGhFjtAAD2Y+wASUoaeIs63jdDgfvv+Oxg6uXq9INSnVn6pIK+d80VB9st/snPtHHLb/WlK66QJF3192laMPt7WvDUc/r9jp3qmHqZvvH1m3T/vROUkMAjWNqEG25I6NZLl/3rFOlvHjOX1H+wLhszTQlduxmsDKb875/+rMWPzdDAfteHj5X9/AV98GGt1i5/VikpyZq38Bm9tGa9xo2+o4Uz4YLivfM9cOCANm7cqA8//FAJCQnyer269dZb1a9fPzvqi38pHZR63wydfek5dZwy87PD38rXmZ+VqOP9sw0WBxMaGhq0d/8BLVvxih4/ekzpvXvp+9O/oz/ve0+3D8lShw4pkqRvZmZo2YuvEL5tFeMz3xb/PbNixQo9+OCDkqR+/fqpb9++kqTZs2dr6dKll746B0gt+K4atr6h4JGD5xw//eMfKHhon6GqYFLt8Trd9NUBKvrORK1ZvkT9+35ZRTMeU//r/0kV//UbnT59Ro2NjdqwaYuOH68zXW78skKt3wxosfNdvny51q1bp9TU1HOOT5o0Sfn5+Zo8efIlLS7epdx2hxQMqvG3FXJ1YbyAT6X17K5nf/TD8P6kcaP0n794UUO/mamTn3yi8VMelPuKTsoZkql3D/jMFRrvYrzzbTF8k5KS1PQ5z8Q8e/askpOTL1lRTpH8z9lypXRQp8eekxKTpZQUdXrsOX1S+qisEx+ZLg+G7HvvkPa9d1B35AwJH7MsKTW1g+4ZM0oPF94nSXpj0xb1Tuthqsy4Z8XzzHfq1KkaMWKEMjIy5PF45HK5VFtbq23btqm4uNiuGuPWJz/87O2lri7ddMUTP9epuVMNVoRYkJDgUslTz+mr/fsqrWd3vbz2Df1jn3/Qb6v+oK1vb9dPFs7VmTNntfzltbp3wr+YLjd+xfNqh7y8PA0ePFhVVVWqra1VKBTSjTfeqKKiInXrxj+jgba49uqr9IPiaSp8ZJ6CoZC6ebpq8bzvy9O1i3b9eZ9GTJiqUCikUXfkKPu2W02XG79ifOzAmyxgFG+ywIW0900Wn8wb2+rPXj5vZbuu1Ras8wXgTDHe+RK+AJyJB+sAgAF0vgBgP6spjlc7AEDcovMFAAOY+QKAAXS+AGA/i/AFAAO44QYABtD5AoABhC8A2M/Gx9a0CeELwJnofAHAgBgPX95JDcCRrKZQq7e2WLhwoWbMmCFJ2rt3r0aOHKmhQ4fq0Ucf/dw3AJ2P8AXgTKEotihVVVVp7dq14f2HH35Yc+bM0caNG2VZllatWhXxHIQvAEeyQlart0AgoOrq6mZbIBBodt4TJ06otLRUU6d++kqwo0eP6uzZs7rhhhskSSNHjlRFRUXE+pj5AnCmKGa+5eXlKisra3a8sLBQRUVF5xybM2eOiouLdezYMUlSbW2tPB5P+O8ej0c1NTURr0n4AnCmKMYJBQUFys/Pb3bc7Xafs7969Wr16NFDGRkZWrNmzaeXCYXkcrnCn7Es65z9CyF8AThSNM92cLvdzYL282zYsEF+v1933nmnPv74Y50+fVoul0t+vz/8mePHj8vr9UY8F+ELwJGspou/1GzZsmXh/16zZo22b9+uBQsWaPjw4dqxY4cGDRqkX/3qV8rMzIx4LsIXgDPZ+DjfJ598UrNmzdKpU6fUt29fTZw4MeJ3eHU8jOLV8biQ9r46/qO8rFZ/tsvrW9t1rbag8wXgTLH9IgvCF4AzxfhbhAhfAM5kRf6Fr1GELwBHovMFAAMIXwAwwYr8KzOTCF8AjkTnCwAGWCE6XwCwXShI+AKA7Rg7AIABjB0AwIAYf3M84QvAmeh8AcAAbrgBgAF0vgBggMUv3ADAfiw1AwADQnS+AGA/xg4AYACrHQDAAFY7AIABzHwBwABmvgBgAM92AAADGDsAgAEhbrh9psuKvXZeDnHg5KTfmC4BMSo58+p2fZ/OFwAM4IYbABhA5wsABsT4YgfCF4AzBUMJpktoUWxXBwBtFIpii8bTTz+tYcOGKTc3V8uWLZMkVVZWKi8vT9nZ2SotLW3Veeh8ATiSpYs/892+fbu2bdum1157TU1NTRo2bJgyMjI0c+ZMvfDCC+rRo4emTJmirVu3Kisrq8Vz0fkCcKSQ1fqttQYPHqzly5crKSlJH330kYLBoAKBgNLT09W7d28lJSUpLy9PFRUVEc9F5wvAkUJRdL6BQECBQKDZcbfbLbfbfc6x5ORkPfPMM1q6dKlycnJUW1srj8cT/rvX61VNTU3Ea9L5AnAkS65Wb+Xl5RoyZEizrby8/HPPPX36dFVVVenYsWPy+XxyuT4Lesuyztm/EDpfAI4UjKLzLSgoUH5+frPj53e9Bw4cUENDg6677jqlpqYqOztbFRUVSkxMDH/G7/fL6/VGvCadLwBHima1g9vtVlpaWrPt/PCtrq7WrFmz1NDQoIaGBm3evFljxozRoUOHdPjwYQWDQa1fv16ZmZkR66PzBeBIl+LlxVlZWdq1a5dGjBihxMREZWdnKzc3V1deeaWKiopUX1+vrKws5eTkRDyXy7Lse+plUkovuy6FOHHyrfmmS0CMSs28p13ff6Pb2FZ/NrdmZbuu1RZ0vgAcKcafKEn4AnCmaJaamUD4AnCkoOkCIiB8AThSqBVrbU0ifAE4Eo+UBAADLsVSs4uJ8AXgSKx2AAADovl5sQmELwBHovMFAAOY+QKAAax2AAADGDsAgAGMHQDAgCCdLwDYj84XAAwgfAHAAFY7AIABrHYAAAMYOwCAATxMHQAMYOwAAAYwdgAAA1jtAAAGhGI8fglfAI7EDTcAMICZLwAYwGoHADCAmS8AGBDb0Uv4AnAoZr4AYEAwxnvfBNMFAMClEIpii0ZZWZlyc3OVm5urRYsWSZIqKyuVl5en7OxslZaWtuo8hC8ARwrJavXWWpWVlfrd736ntWvXat26ddqzZ4/Wr1+vmTNnasmSJdqwYYN2796trVu3RjwX4QvAkawottbyeDyaMWOGUlJSlJycrGuuuUY+n0/p6enq3bu3kpKSlJeXp4qKiojnYuYLwJGiGScEAgEFAoFmx91ut9xud3j/2muvDf+3z+fTm2++qQkTJsjj8YSPe71e1dTURLwm4QvAkaK54VZeXq6ysrJmxwsLC1VUVNTs+P79+zVlyhQ98sgjSkxMlM/nC//Nsiy5XJF/4UH42mzxwjkaNWq46v56QpL07rsHNG78NMNVwW4/WrVZv97xF7k7XiZJuqp7F82/N08lKzfpf/YfkSR9/SvX6Lujb1NiAtPBtohmlltQUKD8/Pxmx/+26/1/O3bs0PTp0zVz5kzl5uZq+/bt8vv94b/7/X55vd6I1yR8bZaRcaPGT7hfVdveMV0KDNp5oFol992pG/qkhY+9sGm7/nrytF6Zd59ClqXJi17Qpj/s1e039TVYafyKZpZ7/njhQo4dO6YHHnhApaWlysjIkCQNGDBAhw4d0uHDh5WWlqb169dr1KhREc9F+NooJSVFN9zQVw89NE1XX52u/fsP6nsPzdORIx+YLg02amhs0l/er9EvNm5T9S9PKL3blXroriG6O3uwxnxzkBISXPpr4LROnq7Xly5PNV1u3LoUPy9+/vnnVV9fr5KSkvCxMWPGqKSkREVFRaqvr1dWVpZycnIinovwtVHPnt20ZcvbmjN3kfbs2afvPThVa15dpq8NHmq6NNjIf+KUvvbldBWOyNI1PbuqfNPv9d3/eFUvzZ6k5KREPf3qFr20ZYeuT++hgdf2Nl1u3LoUv3CbNWuWZs2a9bl/e+2116I6l8uyrAv+38MHH7TckfXs2TOqiyWl9Irq818Edcf/oq/e+G35fEdMl2LEybfmmy7BOMuy9M/Tf6xVc+5VL09nSVJjU1CPL39Tliw9MTnPcIVmpGbe067v/9tVo1v92Z/7XmnXtdqixc53ypQp8vl88nq9Oj+jXS6XNm/efEmLc5p+/a5T//7Xa8WKV8PHXC6XGhubDFYFu71bXat3j9RoeEa/8DHLko7VBdQUDCq9exclJyXqjq/308KVvzZYaXyL9Z8Xtxi+K1eu1Lhx4zR37lwNGjTIrpocKxQK6akfP663394un++Ipk4p0J/+tFdHjx4zXRpslOByaeFLb2lgn97q5emsVf/9R12b5tEf97+vXQc/0FMPjFaCy6UNv9+jr3053XS5cSuuH6zTqVMnPfHEE1q9ejXhexHs2bNP/148W+vW/kKJiYk6Wn1M4+++33RZsFmfXh7NGPttTS9brVDIkvfvrlDJfXeq65c6adHLv9Zdjz2vhASXBvZJ0/T8b5guN26FLjxRjQktznwvNma+OB8zX1xIe2e+E9JHtvqzvzy8pl3XagtWOwBwJN5kAQAGWIQvANivifAFAPvR+QKAAXG91AwA4pWNC7nahPAF4EisdgAAA+L658UAEK/ofAHAAGa+AGAAqx0AwADW+QKAAcx8AcCAoBXbgwfCF4AjMXYAAANi/WHqhC8AR4rt6CV8ATgUN9wAwADCFwAMYLUDABjAagcAMIBnOwCAAcx8AcAAOl8AMCAY4881I3wBOBK/cAMAA2J9tUOC6QIA4FIIWVart2idOnVKw4cPV3V1tSSpsrJSeXl5ys7OVmlpaavOQfgCcCQriv9FY+fOnRo7dqx8Pp8k6ezZs5o5c6aWLFmiDRs2aPfu3dq6dWvE8xC+ABwpms43EAiourq62RYIBJqdd9WqVZo7d668Xq8kadeuXUpPT1fv3r2VlJSkvLw8VVRURKyPmS8AR4rm58Xl5eUqKytrdrywsFBFRUXnHJs/f/45+7W1tfJ4POF9r9ermpqaiNckfAE4UjTjhIKCAuXn5zc77na7I343FArJ5XJ9dl3LOmf/QghfAI5kRdH5ut3uVgXt5+nevbv8fn943+/3h0cSLWHmC8CRQrJavbXHgAEDdOjQIR0+fFjBYFDr169XZmZmxO/R+QJwJLt+XtyhQweVlJSoqKhI9fX1ysrKUk5OTsTvuSwbfwCdlNLLrkshTpx8a37kD+ELKTXznnZ9P+3Kr7T6s9V1u9t1rbag8wXgSMEQz3YAANvF+s+LCV8AjsQjJQHAAB6mDgAG0PkCgAHccAMAAxg7AIABjB0AwABeIwQABrDOFwAMoPMFAANCUTxS0gTCF4AjccMNAAyI9fC19ZGSAIBP8SYLADCA8AUAAwhfADCA8AUAAwhfADCA8AUAAwhfADCA8AUAAwhfADCA8LXZ66+/rmHDhik7O1srVqwwXQ5iyKlTpzR8+HBVV1ebLgU2IHxtVFNTo9LSUr344otat26dXn75Zb333numy0IM2Llzp8aOHSufz2e6FNiE8LVRZWWlbr75ZnXu3FkdO3bU0KFDVVFRYbosxIBVq1Zp7ty58nq9pkuBTXiqmY1qa2vl8XjC+16vV7t27TJYEWLF/PnzTZcAm9H52igUCsnlcoX3Lcs6Zx/AFwfha6Pu3bvL7/eH9/1+P//MBL6gCF8b3XLLLaqqqlJdXZ3OnDmjTZs2KTMz03RZAAxg5mujbt26qbi4WBMnTlRjY6NGjx6t/v37my4LgAG8yQIADGDsAAAGEL4AYADhCwAGEL4AYADhCwAGEL4AYADhCwAGEL4AYMD/AXThoIVN0z2QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = test_df['P/NP']\n",
    "y_pred = test_df['Predicted by Rel and Syn Count / Len']\n",
    "conmat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(conmat)\n",
    "sns.set()\n",
    "\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "\n",
    "print('The classification report for Related and Synonym Count / Len is')\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some testing, it seems the most accurate result we can get comes when the threshold is set such that if the number of Synonym pairs or Related pairs are below the mean for Non-Puns, the sentence is a pun.\n",
    "\n",
    "---\n",
    "\n",
    "**Original Hypothesis**: Puns will have more related pairs of words than Non-Puns\n",
    "\n",
    "**Finding**: In reality, it seems that the more related pairs of words a sentence has, the more likely that it is **not** a pun.\n",
    "\n",
    "The reason our Hypothesis might have failed is because in reality, sentences tend to use very closely related words in order to describe situations. On the other hand, with puns, there *are* closely related pairs of words, but those words also tend to be very unrelated to other words.\n",
    "\n",
    "---\n",
    "\n",
    "For example:\n",
    "\n",
    "Pun - \"My **phone** has had to wear **glasses** ever since it lost its **contacts**.\"\n",
    "* While **contacts** is related to both phone and glasses, phone and glasses are completely unrelated\n",
    "\n",
    "Non-Pun - \"**Life** is the **flower** for which **love** is the **honey**\"\n",
    "* Here, Life is related to Love and Flower is related to Honey. However, Life is also loosely related to Flower. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Converging meanings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
